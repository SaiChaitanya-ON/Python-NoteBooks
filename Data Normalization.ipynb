{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "49839795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pylab as pl\n",
    "from scipy.stats import norm\n",
    "import statistics\n",
    "\n",
    "%store -r Data_Set\n",
    "\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "import embedding_retriever as er\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7db2bdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('43dd0641-8b1c-59b6-bf31-34e51c555761', {'Year': [2018, 2017, 2016, 2015, 2014], 'Sector': '\"713210\"', 'R': [72573907, 72318836, 68191837, 62567540, 61064684], 'EBITDA': [5334401, 6084786, 7645315, 5499604, 4643608], 'NDE': [-0.0, 0.2, 0.7, -0.0, 0.9], 'TDE': [23, 35, 49, 23, 43]}) \n",
      "\n",
      "('6e8f931b-af06-5f86-a4ba-2176912af400', {'Year': [2018, 2017, 2016, 2015, 2014], 'Sector': '\"524126\"', 'R': [1466336000, 1275713000, 1179796000, 1052805000, 1000062999], 'EBITDA': [65173000, 98141000, 129672000, 138775000, 150208000], 'NDE': [1.0, 1.6, 1.5, 1.3, 1.1], 'TDE': [22, 21, 22, 24, 25]}) \n",
      "\n",
      "('51b7c557-7662-577b-a635-5a268961910a', {'Year': [2020, 2019, 2018, 2017, 2016], 'Sector': '\"45411\"', 'R': [63324, 163740, 159672, 86075, 67964], 'EBITDA': [-5385, -237467, -488578, -765968, -540626], 'NDE': [-92.0, -2.0, -1.0, 0.0, 0.2], 'TDE': [-68, -69, -126, 105, 36]}) \n",
      "\n",
      "('b735464c-bbb0-5740-ac31-3b43884bfedf', {'Year': [2016, 2015, 2014, 2013, 2012], 'Sector': '\"334519\"', 'R': [63311000, 60754000, 64474999, 57108000, 49941000], 'EBITDA': [9641000, 7767000, 9444000, 7216000, 6986000], 'NDE': [-0.0, -0.0, -0.0, 0.5, 0.5], 'TDE': [0, 8, 13, 22, 34]}) \n",
      "\n",
      "('6800bdd9-4bcf-59db-81fc-725616185422', {'Year': [2020, 2019, 2018, 2017, 2016], 'Sector': '\"518210\"', 'R': [439100999, 485778000, 564754000, 504750000, 419221000], 'EBITDA': [104089000, 121486999, 64025999, 119299000, 104363999], 'NDE': [1.0, 3.5, 5.8, 5.5, 5.7], 'TDE': [181, 461, 492, -4733, -831]}) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for i in Data_Set.items():\n",
    "    print(i,\"\\n\")\n",
    "    n += 1\n",
    "    if n == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6a86cfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7440\n"
     ]
    }
   ],
   "source": [
    "print(len(Data_Set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c1ed02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "H = []\n",
    "\n",
    "for i in Data_Set.values():\n",
    "    for j in i[5]:\n",
    "        H.append(j)\n",
    "H.sort()\n",
    "fit = stats.norm.pdf(H, np.mean(H), np.std(H))  #this is a fitting indeed\n",
    "\n",
    "plt.xlim(-5000000000,50000000000)\n",
    "plt.plot(H,fit)\n",
    "plt.title(\"Revenue\")\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d76252d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "H = []\n",
    "\n",
    "for i in Data_Set.values():\n",
    "    for j in i[7]:\n",
    "        H.append(j)\n",
    "H.sort()\n",
    "fit = stats.norm.pdf(H, np.mean(H), np.std(H))  #this is a fitting indeed\n",
    "\n",
    "plt.xlim(-5000000000,5000000000)\n",
    "plt.plot(H,fit)\n",
    "plt.title(\"EBITDA\")\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2de32f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Shifted_MinMax_Normalize(Data, normalizedMin, normalizedMax):\n",
    "    \n",
    "    Data_Set = Data.copy()\n",
    "    \n",
    "    R_Min = list(Data_Set.values())[0][5][1]\n",
    "    R_Max = list(Data_Set.values())[0][5][1]\n",
    "    \n",
    "    E_Min = list(Data_Set.values())[0][7][1]\n",
    "    E_Max = list(Data_Set.values())[0][7][1]\n",
    "    \n",
    "    for i in Data_Set.values():\n",
    "        for j in range(len(i[5])):\n",
    "            if i[5][j] < R_Min:\n",
    "                R_Min = i[5][j]\n",
    "            elif i[5][j] > R_Max:\n",
    "                R_Max = i[5][j]\n",
    "                \n",
    "    print(\"R_Min\", R_Min)\n",
    "    \n",
    "    for i in Data_Set.values():\n",
    "        for j in range(len(i[7])):\n",
    "            if i[7][j] < E_Min:\n",
    "                E_Min = i[7][j]\n",
    "            elif i[7][j] > E_Max:\n",
    "                E_Max = i[7][j]\n",
    "                \n",
    "    print(\"E_Min\", E_Min)\n",
    "                \n",
    "    for i in Data_Set.values():\n",
    "        for j in range(len(i[5])):              \n",
    "            mx = (i[5][j] - R_Min)/(R_Max - R_Min)\n",
    "            preshiftNormalized = mx*(normalizedMax-normalizedMin);\n",
    "            i[5][j] = round(preshiftNormalized + normalizedMin, 6)\n",
    "    \n",
    "    for i in Data_Set.values():\n",
    "        for j in range(len(i[7])):              \n",
    "            mx = (i[7][j] - E_Min)/(E_Max - E_Min)\n",
    "            preshiftNormalized = mx*(normalizedMax-normalizedMin);\n",
    "            i[7][j] = round(preshiftNormalized + normalizedMin, 6)\n",
    "\n",
    "    return Data_Set\n",
    "\n",
    "\n",
    "# SMMN_Data_Set = Shifted_MinMax_Normalize(Data_Set,0,10)\n",
    "\n",
    "# n = 0\n",
    "# for i in SMMN_Data_Set.items():\n",
    "#     print(i,\"\\n\")\n",
    "#     n += 1\n",
    "#     if n == 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "257d4fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sector_List = [\"511210\", \"2111\", \"325412\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cbcd9743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Set_Normalised(Data_Set):\n",
    "    \n",
    "    Data_Set_N = {}\n",
    "    \n",
    "    for i in Data_Set.items():\n",
    "        ID,data = i\n",
    "        \n",
    "        Data_Set_N[ID] = {'Sector': data['Sector']}\n",
    "        \n",
    "        Data_Set_N[ID].update({'Revenue' : []})\n",
    "        for j in data['R']:\n",
    "            if j < 0:\n",
    "                Data_Set_N[ID]['Revenue'].append(0)\n",
    "            elif j in range(0,5000000):\n",
    "                Data_Set_N[ID]['Revenue'].append(0.1)\n",
    "            elif j in range(5000000,10000000):\n",
    "                Data_Set_N[ID]['Revenue'].append(0.2)\n",
    "            elif j in range(10000000,50000000):\n",
    "                Data_Set_N[ID]['Revenue'].append(0.3)\n",
    "            elif j in range(50000000,100000000):\n",
    "                Data_Set_N[ID]['Revenue'].append(0.4)\n",
    "            elif j in range(100000000,500000000):\n",
    "                Data_Set_N[ID]['Revenue'].append(0.5)\n",
    "            elif j in range(500000000,1000000000):\n",
    "                Data_Set_N[ID]['Revenue'].append(0.6)\n",
    "            elif j in range(1000000000,5000000000):\n",
    "                Data_Set_N[ID]['Revenue'].append(0.7)\n",
    "            elif j in range(5000000000,10000000000):\n",
    "                Data_Set_N[ID]['Revenue'].append(0.8)\n",
    "            elif j > 10000000000:\n",
    "                Data_Set_N[ID]['Revenue'].append(0.9)\n",
    "                \n",
    "        \n",
    "        Data_Set_N[ID].update({'EBITDA' : []})\n",
    "        for j in data['EBITDA']:\n",
    "            if j < 0:\n",
    "                Data_Set_N[ID]['EBITDA'].append(0)\n",
    "            elif j in range(0,5000000):\n",
    "                Data_Set_N[ID]['EBITDA'].append(0.1)\n",
    "            elif j in range(5000000,10000000):\n",
    "                Data_Set_N[ID]['EBITDA'].append(0.2)\n",
    "            elif j in range(10000000,50000000):\n",
    "                Data_Set_N[ID]['EBITDA'].append(0.3)\n",
    "            elif j in range(50000000,100000000):\n",
    "                Data_Set_N[ID]['EBITDA'].append(0.4)\n",
    "            elif j in range(100000000,500000000):\n",
    "                Data_Set_N[ID]['EBITDA'].append(0.5)\n",
    "            elif j in range(500000000,1000000000):\n",
    "                Data_Set_N[ID]['EBITDA'].append(0.6)\n",
    "            elif j in range(1000000000,5000000000):\n",
    "                Data_Set_N[ID]['EBITDA'].append(0.7)\n",
    "            elif j in range(5000000000,10000000000):\n",
    "                Data_Set_N[ID]['EBITDA'].append(0.8)\n",
    "            elif j > 10000000000:\n",
    "                Data_Set_N[ID]['EBITDA'].append(0.9)\n",
    "                \n",
    "            \n",
    "        Data_Set_N[ID].update({'NDE' : []})\n",
    "        for j in data['NDE']:\n",
    "            if j < 0 or j > 1:\n",
    "                Data_Set_N[ID]['NDE'].append(0)\n",
    "            else:\n",
    "                Data_Set_N[ID]['NDE'].append(round(1 - j,4))\n",
    "          \n",
    "        \n",
    "        Data_Set_N[ID].update({'TDE' : []})\n",
    "        for j in data['TDE']:\n",
    "            if j < 0 or j > 100:\n",
    "                Data_Set_N[ID]['TDE'].append(0)\n",
    "            else:\n",
    "                Data_Set_N[ID]['TDE'].append(round(100 - j,4)/100)\n",
    "                \n",
    "        \n",
    "        \n",
    "    return Data_Set_N\n",
    "    \n",
    "    \n",
    "Data_Set_N = Data_Set_Normalised(Data_Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "956e6b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'Data_Set_N' (dict)\n"
     ]
    }
   ],
   "source": [
    "%store Data_Set_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "879c20eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('43dd0641-8b1c-59b6-bf31-34e51c555761', {'Sector': '\"713210\"', 'Revenue': [0.4, 0.4, 0.4, 0.4, 0.4], 'EBITDA': [0.2, 0.2, 0.2, 0.2, 0.1], 'NDE': [1.0, 0.8, 0.3, 1.0, 0.1], 'TDE': [0.77, 0.65, 0.51, 0.77, 0.57]}) \n",
      "\n",
      "('6e8f931b-af06-5f86-a4ba-2176912af400', {'Sector': '\"524126\"', 'Revenue': [0.7, 0.7, 0.7, 0.7, 0.7], 'EBITDA': [0.4, 0.4, 0.5, 0.5, 0.5], 'NDE': [0.0, 0, 0, 0, 0], 'TDE': [0.78, 0.79, 0.78, 0.76, 0.75]}) \n",
      "\n",
      "('51b7c557-7662-577b-a635-5a268961910a', {'Sector': '\"45411\"', 'Revenue': [0.1, 0.1, 0.1, 0.1, 0.1], 'EBITDA': [0, 0, 0, 0, 0], 'NDE': [0, 0, 0, 1.0, 0.8], 'TDE': [0, 0, 0, 0, 0.64]}) \n",
      "\n",
      "('b735464c-bbb0-5740-ac31-3b43884bfedf', {'Sector': '\"334519\"', 'Revenue': [0.4, 0.4, 0.4, 0.4, 0.3], 'EBITDA': [0.2, 0.2, 0.2, 0.2, 0.2], 'NDE': [1.0, 1.0, 1.0, 0.5, 0.5], 'TDE': [1.0, 0.92, 0.87, 0.78, 0.66]}) \n",
      "\n",
      "('6800bdd9-4bcf-59db-81fc-725616185422', {'Sector': '\"518210\"', 'Revenue': [0.5, 0.5, 0.6, 0.6, 0.5], 'EBITDA': [0.5, 0.5, 0.4, 0.5, 0.5], 'NDE': [0.0, 0, 0, 0, 0], 'TDE': [0, 0, 0, 0, 0]}) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for i in Data_Set_N.items():\n",
    "    print(i,\"\\n\")\n",
    "    n += 1\n",
    "    if n == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aa5054d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7440\n"
     ]
    }
   ],
   "source": [
    "print(len(Data_Set_N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e571bea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e809884c-8121-52f8-806c-6d1b86adae2e', {'Sector': '511210', 'Description': 'Forescout Technologies, Inc. provides device visibility and control solution', 'Description Emb': array([[ 5.7600e-01,  7.7410e-01,  8.9120e-01,  7.5210e-01,  1.1872e+00,\n",
      "         6.7610e-01,  2.7440e-01,  1.0162e+00,  8.5640e-01,  6.3140e-01,\n",
      "         4.3880e-01,  2.7280e-01,  7.2570e-01,  1.8619e+00,  5.3800e-01,\n",
      "         1.2024e+00,  2.8100e-01,  9.9480e-01,  5.8390e-01,  6.9910e-01,\n",
      "         4.3010e-01,  4.7800e-02,  7.4360e-01,  1.4881e+00,  9.0410e-01,\n",
      "         6.2760e-01,  2.2730e-01,  3.8470e-01,  2.5190e-01,  8.9040e-01,\n",
      "         7.7160e-01,  9.8240e-01,  9.1970e-01,  4.1410e-01,  5.5550e-01,\n",
      "         3.5180e-01,  3.0050e-01,  5.6390e-01,  6.5250e-01,  7.1340e-01,\n",
      "         4.6900e-02,  3.4330e-01,  1.1250e+00,  4.7230e-01,  9.4290e-01,\n",
      "         6.1250e-01,  1.1234e+00,  1.2333e+00,  7.3890e-01,  9.0870e-01,\n",
      "         7.8400e-02,  1.4007e+00,  1.0414e+00,  4.7210e-01,  9.2410e-01,\n",
      "         9.7060e-01,  9.2720e-01,  2.3050e-01,  1.4018e+00,  5.3680e-01,\n",
      "         4.3480e-01,  4.9840e-01,  6.7420e-01,  6.8190e-01,  8.4540e-01,\n",
      "         2.9210e-01,  8.2160e-01,  1.1460e+00,  8.7200e-02,  6.0140e-01,\n",
      "         7.9110e-01,  9.6290e-01,  4.5830e-01,  8.3580e-01,  3.8720e-01,\n",
      "         7.5700e-01,  3.1710e-01,  1.3720e+00,  8.7490e-01,  9.2380e-01,\n",
      "         7.2900e-01,  1.3657e+00,  1.0210e+00,  8.7610e-01,  7.7440e-01,\n",
      "         1.1606e+00,  5.3500e-01,  9.9230e-01,  4.3440e-01,  1.1933e+00,\n",
      "         6.9220e-01,  6.6470e-01,  1.1385e+00,  8.9600e-01,  8.7220e-01,\n",
      "         6.4970e-01,  4.2570e-01,  3.2480e-01,  6.8420e-01,  1.4858e+00,\n",
      "         9.2330e-01,  4.1750e-01,  7.8430e-01,  6.1980e-01,  9.1870e-01,\n",
      "         9.1590e-01,  1.0318e+00,  9.8090e-01,  7.7070e-01,  8.4010e-01,\n",
      "         1.2458e+00,  4.1190e-01,  6.5020e-01,  5.4460e-01,  4.4960e-01,\n",
      "         1.1755e+00,  5.3760e-01,  5.9770e-01,  1.1536e+00,  2.9140e-01,\n",
      "         8.1900e-01,  1.1130e+00,  6.3040e-01,  1.7215e+00,  3.7850e-01,\n",
      "         9.7720e-01,  6.9410e-01,  9.0290e-01,  1.1049e+00,  5.8460e-01,\n",
      "         8.3780e-01,  7.4560e-01,  5.6700e-01,  6.2250e-01,  5.3380e-01,\n",
      "         9.3950e-01,  6.7550e-01,  2.9540e-01,  1.2010e+00,  5.2570e-01,\n",
      "         8.8280e-01,  7.2350e-01,  1.0542e+00,  7.3840e-01,  8.1700e-01,\n",
      "         4.7920e-01,  1.0844e+00,  5.5200e-01,  4.9560e-01,  1.1218e+00,\n",
      "         1.1230e+00,  1.1643e+00,  4.0830e-01,  2.2570e-01,  6.6510e-01,\n",
      "         4.9910e-01,  5.8850e-01,  1.1248e+00,  1.4639e+00,  5.1860e-01,\n",
      "         1.2309e+00,  1.2955e+00,  1.6110e-01,  1.0656e+00,  4.1910e-01,\n",
      "         8.1720e-01,  9.3310e-01,  1.0777e+00,  5.9950e-01,  1.1889e+00,\n",
      "         9.7320e-01,  5.3500e-01,  9.6830e-01,  4.8740e-01,  1.9520e+00,\n",
      "         7.8390e-01,  7.2300e-01,  3.1510e-01,  4.4460e-01,  9.1090e-01,\n",
      "        -1.4400e-02,  1.1405e+00,  1.1290e+00,  7.6150e-01,  9.1070e-01,\n",
      "         5.4940e-01,  1.0711e+00,  3.1070e-01,  1.1933e+00,  5.5920e-01,\n",
      "        -1.0900e-02,  5.6600e-02,  6.0800e-01,  6.1850e-01,  5.8500e-01,\n",
      "         4.2840e-01,  6.2900e-01,  7.2070e-01,  9.4650e-01,  6.7540e-01,\n",
      "         7.7120e-01,  5.7010e-01,  8.3410e-01,  7.3860e-01,  4.1770e-01,\n",
      "         3.3158e+00,  1.0464e+00,  2.0730e-01,  2.0310e-01,  4.3520e-01,\n",
      "         5.7950e-01,  6.9700e-01,  6.6780e-01,  9.6780e-01,  1.2082e+00,\n",
      "         6.1770e-01,  3.4950e-01,  1.0570e+00,  6.5830e-01,  6.7420e-01,\n",
      "         1.3153e+00,  7.3630e-01,  3.1010e-01,  9.5450e-01,  2.4220e-01,\n",
      "         1.5653e+00,  6.3850e-01,  6.4320e-01,  1.0112e+00,  7.6960e-01,\n",
      "         7.5980e-01,  5.2460e-01,  1.2976e+00,  4.9200e-01,  6.1030e-01,\n",
      "         7.2920e-01,  2.6950e-01,  4.0140e-01,  1.2653e+00,  4.7830e-01,\n",
      "         2.8000e-03,  1.1209e+00,  1.1700e+00,  3.1010e-01,  1.7323e+00,\n",
      "         5.1880e-01,  1.2695e+00,  1.0123e+00,  9.0950e-01,  3.4530e-01,\n",
      "         7.8430e-01,  8.1440e-01,  5.9650e-01,  3.6210e-01,  1.3203e+00,\n",
      "         5.6290e-01,  6.0680e-01,  9.4110e-01,  3.7220e-01,  1.1645e+00,\n",
      "         8.8780e-01,  5.7770e-01,  1.3478e+00,  9.7650e-01,  8.8320e-01,\n",
      "         1.1280e-01,  9.5050e-01,  1.0435e+00,  6.6050e-01,  7.9780e-01,\n",
      "         9.2910e-01,  4.2970e-01,  9.0940e-01,  1.2130e+00,  4.0940e-01,\n",
      "         4.4760e-01,  9.6560e-01,  8.3330e-01,  7.2510e-01,  4.5560e-01,\n",
      "         1.0214e+00,  6.9840e-01,  9.0230e-01,  6.2630e-01,  1.0190e+00,\n",
      "         4.9170e-01,  1.0699e+00,  7.9950e-01,  7.0030e-01,  5.1890e-01,\n",
      "         4.4050e-01,  9.4870e-01,  2.6390e-01,  8.7030e-01,  1.0820e+00,\n",
      "         8.4030e-01,  7.0340e-01,  1.1469e+00,  6.1240e-01,  5.1810e-01,\n",
      "         7.3720e-01,  3.7560e-01,  1.2739e+00,  5.2860e-01,  7.5730e-01,\n",
      "         3.8030e-01,  1.2256e+00,  4.7200e-01,  2.2187e+00,  3.4130e-01,\n",
      "         1.0785e+00,  3.1040e-01,  1.7760e+00,  4.9960e-01,  1.0523e+00,\n",
      "         5.1570e-01,  5.2920e-01,  1.1292e+00,  1.0032e+00,  5.1970e-01,\n",
      "         9.4860e-01,  6.3230e-01,  5.0020e-01,  1.5533e+00,  1.0085e+00,\n",
      "         2.3360e-01,  8.1710e-01,  7.7710e-01,  3.4850e-01,  4.9970e-01,\n",
      "         6.9240e-01,  9.1590e-01,  1.0334e+00,  6.8830e-01,  5.7950e-01,\n",
      "         1.2182e+00,  5.3780e-01,  9.8600e-01,  6.7600e-01,  2.7790e-01,\n",
      "         4.2460e-01,  1.3832e+00,  2.7840e-01,  2.0770e-01,  5.3380e-01,\n",
      "         8.5540e-01,  1.4485e+00,  2.9080e-01,  1.2281e+00,  1.0099e+00,\n",
      "         5.2040e-01,  7.8100e-01,  1.3666e+00,  2.8720e-01,  9.0520e-01,\n",
      "         7.1300e-01,  7.1780e-01,  5.7290e-01,  1.1051e+00,  4.7660e-01,\n",
      "         1.0445e+00,  5.0370e-01,  5.6260e-01,  2.7160e-01,  9.7900e-01,\n",
      "         9.3370e-01, -2.1180e-01,  9.0490e-01,  9.2150e-01,  5.4940e-01,\n",
      "         5.2150e-01,  5.4680e-01,  1.1553e+00,  6.2190e-01,  4.6240e-01,\n",
      "         8.2640e-01,  1.5880e-01,  4.3320e-01,  5.5540e-01,  1.4502e+00,\n",
      "         9.2320e-01,  1.2010e-01,  6.5630e-01,  1.1200e-01,  8.3240e-01,\n",
      "         4.6360e-01,  7.4280e-01,  4.3110e-01,  4.2110e-01,  1.0456e+00,\n",
      "         9.4090e-01,  1.0344e+00,  8.9430e-01,  6.2300e-01,  5.5440e-01,\n",
      "         5.8320e-01,  5.5400e-01,  5.6450e-01,  9.5790e-01,  1.3408e+00,\n",
      "         1.2076e+00,  5.9230e-01,  1.1063e+00,  7.5740e-01,  8.6460e-01,\n",
      "         9.1860e-01,  5.0820e-01,  4.1310e-01,  3.8450e-01,  6.9680e-01,\n",
      "         1.0080e+00,  4.1120e-01,  1.8770e-01,  1.1646e+00,  4.6550e-01,\n",
      "         3.1110e-01,  1.0025e+00,  7.1630e-01,  5.7210e-01,  6.6520e-01,\n",
      "         9.1890e-01,  7.5160e-01,  3.3760e-01,  8.5520e-01,  4.6540e-01,\n",
      "         1.3953e+00,  6.9500e-01,  4.7210e-01,  5.4020e-01,  9.5140e-01,\n",
      "         6.7350e-01,  8.4340e-01,  8.0580e-01,  2.3180e-01,  2.1380e-01,\n",
      "         1.3490e+00,  8.6330e-01,  3.5910e-01,  2.8330e-01,  1.0511e+00,\n",
      "         7.1400e-01,  9.2300e-01,  6.7950e-01,  4.0390e-01,  7.9200e-01,\n",
      "         7.0470e-01,  5.6330e-01,  6.6510e-01,  9.0130e-01,  1.5015e+00,\n",
      "         1.0804e+00,  6.5480e-01,  1.1089e+00,  1.0939e+00,  9.0820e-01,\n",
      "         1.1964e+00,  4.2350e-01,  9.4450e-01,  6.2310e-01,  5.5880e-01,\n",
      "         7.2210e-01,  9.5490e-01,  1.0526e+00,  7.0280e-01,  9.5210e-01,\n",
      "         7.1920e-01,  7.0870e-01,  2.4800e-01,  5.6740e-01,  9.8270e-01,\n",
      "         8.2170e-01,  3.6460e-01,  6.4430e-01,  1.3420e+00,  4.7440e-01,\n",
      "         8.1930e-01,  1.2287e+00,  1.0178e+00,  1.3057e+00,  1.2918e+00,\n",
      "         5.9900e-01,  2.1250e-01,  5.8650e-01,  1.0946e+00,  9.0320e-01,\n",
      "         3.1690e-01,  5.0270e-01,  1.0152e+00,  6.8550e-01,  5.8480e-01,\n",
      "         2.1110e-01,  7.6070e-01,  1.2690e+00,  9.7130e-01,  6.4010e-01,\n",
      "         5.3320e-01,  6.4290e-01,  1.0131e+00,  1.2535e+00,  7.1470e-01,\n",
      "         2.5200e-02,  7.6080e-01,  6.6500e-01,  2.4600e-01,  9.3510e-01,\n",
      "         1.0354e+00,  5.0190e-01,  7.5520e-01,  7.1370e-01,  6.9830e-01,\n",
      "         2.4150e-01,  7.2770e-01,  3.7760e-01,  2.5250e-01,  1.1719e+00,\n",
      "         9.6100e-01,  1.1740e+00,  8.5170e-01,  8.3210e-01,  2.0300e-01,\n",
      "         1.1035e+00,  6.4990e-01,  9.0050e-01,  9.7050e-01,  1.0788e+00,\n",
      "         4.9230e-01,  4.0490e-01,  7.1130e-01,  1.7480e-01,  4.6590e-01,\n",
      "         8.1730e-01,  1.2643e+00,  4.4080e-01,  8.2270e-01,  6.3350e-01,\n",
      "         1.0466e+00,  5.8480e-01,  3.4320e-01,  8.5010e-01,  7.6360e-01,\n",
      "         4.5630e-01,  6.2720e-01,  1.0761e+00,  1.0029e+00,  3.3410e-01,\n",
      "         5.6610e-01,  9.6620e-01,  7.8480e-01,  3.5470e-01,  2.5560e-01,\n",
      "         1.0210e-01,  4.5030e-01,  8.1040e-01,  9.1460e-01,  6.1620e-01,\n",
      "         1.4756e+00,  8.9080e-01,  1.2784e+00,  1.4688e+00,  7.6650e-01,\n",
      "         6.0900e-01,  8.4830e-01,  9.8240e-01,  1.9830e-01,  2.9830e-01,\n",
      "         7.4720e-01, -1.0610e-01,  5.0880e-01,  3.7700e-01,  8.3920e-01,\n",
      "         7.4390e-01,  7.5260e-01,  7.1500e-01,  5.2620e-01,  7.2530e-01,\n",
      "         6.7260e-01,  5.3020e-01,  7.0020e-01,  6.4890e-01,  6.0890e-01,\n",
      "         1.0717e+00,  7.0130e-01,  4.8130e-01,  6.1100e-01,  1.5806e+00,\n",
      "         4.7080e-01,  8.1690e-01,  7.9200e-01,  1.4291e+00,  6.4050e-01,\n",
      "         1.0094e+00,  1.1721e+00,  1.1607e+00,  4.5010e-01,  1.1807e+00,\n",
      "         7.3770e-01,  7.3410e-01,  6.6320e-01,  8.1150e-01,  1.0932e+00,\n",
      "         3.5180e-01,  9.4070e-01,  8.1260e-01,  8.3070e-01,  1.3986e+00,\n",
      "         8.0480e-01,  1.2073e+00,  1.1972e+00,  6.9200e-01,  1.2515e+00,\n",
      "         9.5400e-01,  7.4240e-01,  5.3750e-01,  1.2665e+00,  6.9840e-01,\n",
      "         2.5890e-01,  3.3490e-01,  1.2690e-01,  5.2180e-01,  4.4610e-01,\n",
      "         1.5166e+00,  1.3049e+00,  3.1930e-01,  9.5590e-01,  5.7550e-01,\n",
      "         7.6090e-01,  5.5750e-01,  9.5440e-01,  3.4820e-01,  7.6800e-01,\n",
      "         1.2224e+00,  2.3340e-01,  8.4420e-01,  1.4350e+00,  5.5250e-01,\n",
      "         1.1074e+00,  5.4370e-01,  8.6720e-01,  8.3790e-01,  8.6750e-01,\n",
      "         8.3100e-01,  1.2162e+00,  7.4260e-01,  6.3430e-01,  1.0879e+00,\n",
      "         6.9420e-01,  7.6500e-01,  8.4300e-01,  1.0318e+00,  7.7100e-01,\n",
      "         1.0767e+00,  1.1279e+00,  4.9650e-01,  9.1280e-01,  1.1768e+00,\n",
      "         9.4770e-01,  8.5640e-01,  1.2069e+00,  1.0414e+00,  6.9420e-01,\n",
      "         1.1166e+00,  1.0302e+00,  1.1527e+00,  8.1560e-01,  5.8100e-01,\n",
      "         1.0824e+00,  9.3610e-01,  1.5946e+00,  7.7890e-01,  5.1320e-01,\n",
      "         1.8855e+00,  3.0120e-01,  7.4400e-02,  9.0230e-01,  1.7598e+00,\n",
      "         6.8810e-01,  6.7310e-01,  5.2810e-01,  6.5080e-01,  1.2834e+00,\n",
      "         8.1780e-01,  7.8000e-01,  8.4130e-01,  8.8650e-01,  9.2230e-01,\n",
      "         4.8700e-01,  5.4610e-01,  1.1601e+00,  6.5480e-01,  4.8520e-01,\n",
      "         3.6160e-01,  4.3190e-01,  1.3086e+00,  5.2940e-01,  1.1080e-01,\n",
      "         6.4440e-01,  1.4059e+00,  8.2120e-01,  3.6670e-01,  4.0180e-01,\n",
      "         1.4283e+00,  7.2150e-01,  1.1477e+00,  4.5650e-01,  9.4150e-01,\n",
      "         5.6310e-01,  8.6840e-01,  1.1127e+00,  1.2129e+00,  8.7100e-01,\n",
      "         3.9930e-01,  6.9920e-01,  4.6770e-01,  9.9130e-01,  1.2173e+00,\n",
      "         1.1433e+00,  2.7070e-01,  5.1060e-01,  6.6080e-01,  1.0994e+00,\n",
      "         4.4390e-01,  4.0930e-01,  3.3710e-01,  1.0307e+00,  6.5950e-01,\n",
      "         1.0234e+00,  6.6390e-01,  7.5370e-01,  1.1330e-01,  3.0330e-01,\n",
      "         6.6380e-01,  6.6050e-01,  8.4200e-01,  1.2946e+00,  1.2279e+00,\n",
      "         8.3190e-01,  5.5240e-01,  7.7570e-01,  5.9720e-01,  6.5490e-01,\n",
      "         7.8890e-01,  1.1310e+00,  6.6500e-01,  7.1470e-01,  3.1800e-01,\n",
      "         1.4710e+00,  6.6550e-01,  3.8510e-01,  1.0170e+00,  3.5720e-01,\n",
      "         9.0600e-01,  6.8980e-01,  1.1729e+00,  2.8030e-01,  6.2090e-01,\n",
      "         1.5680e-01,  2.2714e+00,  1.1464e+00,  1.4325e+00,  2.4270e-01,\n",
      "         4.1200e-01,  6.3080e-01,  9.5640e-01,  7.2870e-01,  8.2780e-01,\n",
      "         3.5740e-01,  5.7520e-01,  7.4890e-01]], dtype=float32)}) \n",
      "\n",
      "('f09eeed5-ae28-59af-92f8-43b51d392970', {'Sector': '2111', 'Description': 'Extraction Oil & Gas, Inc. operates as an independent oil and gas compan', 'Description Emb': array([[ 5.6460e-01,  6.1530e-01,  5.8600e-01,  6.4610e-01,  1.2041e+00,\n",
      "         4.0600e-01,  1.1300e-02,  9.8770e-01,  8.6360e-01,  8.5760e-01,\n",
      "         3.4970e-01,  4.7230e-01,  7.5610e-01,  1.3572e+00,  1.0499e+00,\n",
      "         1.9999e+00,  3.8580e-01,  6.6860e-01,  6.6180e-01,  8.1970e-01,\n",
      "         4.6060e-01,  5.0480e-01,  1.1044e+00,  1.0924e+00,  9.6170e-01,\n",
      "         1.0186e+00,  5.6730e-01,  1.0911e+00, -8.6600e-02,  6.9460e-01,\n",
      "         1.3459e+00,  9.4790e-01,  6.6940e-01,  8.7670e-01,  1.1967e+00,\n",
      "         5.5700e-01,  3.9650e-01,  4.2650e-01,  4.1440e-01,  5.5750e-01,\n",
      "         2.4010e-01,  7.1170e-01,  7.1890e-01,  6.6020e-01,  7.9680e-01,\n",
      "         4.4280e-01,  8.9830e-01,  3.8700e-01,  3.4410e-01,  5.9570e-01,\n",
      "         7.3300e-01,  8.6750e-01,  3.3600e-01,  7.6900e-01,  9.8760e-01,\n",
      "         1.3978e+00,  1.0346e+00,  4.6480e-01,  6.1280e-01,  4.2350e-01,\n",
      "         8.4110e-01,  5.8770e-01,  8.1460e-01,  3.5670e-01,  1.0677e+00,\n",
      "         3.2740e-01,  1.0807e+00,  7.4230e-01,  4.7950e-01,  8.4550e-01,\n",
      "         5.1470e-01,  8.3720e-01,  8.6940e-01,  7.2910e-01,  1.1474e+00,\n",
      "         9.7390e-01,  1.1637e+00,  6.9320e-01,  5.2900e-01,  1.6124e+00,\n",
      "         1.1977e+00,  1.2037e+00,  5.7610e-01,  9.6850e-01,  3.9800e-01,\n",
      "         6.2530e-01,  7.3940e-01,  6.5750e-01,  8.8600e-01,  1.6134e+00,\n",
      "         4.3790e-01,  5.5710e-01,  8.3160e-01,  6.1660e-01,  5.3540e-01,\n",
      "         3.2520e-01,  1.3440e+00,  3.4360e-01,  8.1590e-01,  1.3776e+00,\n",
      "         7.7720e-01,  1.3168e+00,  4.9070e-01,  7.8700e-01,  3.6000e-03,\n",
      "         7.8880e-01,  5.8910e-01,  8.5060e-01,  8.5820e-01,  4.8550e-01,\n",
      "         1.1921e+00,  7.2910e-01,  9.3080e-01,  3.7630e-01,  3.3160e-01,\n",
      "         8.1940e-01,  3.9160e-01,  7.0040e-01,  1.1901e+00,  2.2340e-01,\n",
      "         1.1414e+00,  4.4400e-01,  3.6640e-01,  1.6199e+00,  7.1470e-01,\n",
      "         6.9930e-01,  9.3240e-01,  7.1520e-01,  8.4000e-01,  3.4010e-01,\n",
      "         1.5603e+00,  1.0055e+00,  7.0130e-01,  6.1410e-01,  3.5580e-01,\n",
      "         1.0252e+00,  8.0790e-01,  3.9440e-01,  1.2574e+00,  4.0220e-01,\n",
      "         6.5350e-01,  6.8030e-01,  5.8440e-01,  4.6850e-01,  8.6480e-01,\n",
      "         1.8970e-01,  6.8860e-01,  5.8740e-01,  6.7410e-01,  1.2512e+00,\n",
      "         9.5720e-01,  7.0870e-01,  3.8300e-02,  6.8290e-01,  4.9930e-01,\n",
      "        -2.3350e-01,  1.1223e+00,  5.8240e-01,  1.2082e+00,  5.4730e-01,\n",
      "         1.0091e+00,  6.4170e-01,  1.1048e+00,  9.4080e-01,  5.0880e-01,\n",
      "         3.9670e-01,  7.6010e-01,  8.2700e-01,  2.0330e-01,  6.3760e-01,\n",
      "         3.0900e-01,  5.9400e-02,  1.0721e+00,  5.4500e-01,  9.9190e-01,\n",
      "         1.0337e+00,  3.7500e-01,  1.0684e+00,  9.0820e-01,  9.6710e-01,\n",
      "        -8.5000e-02,  1.6639e+00,  9.1720e-01,  1.4114e+00,  1.2405e+00,\n",
      "         5.8040e-01,  1.3944e+00,  4.8440e-01,  1.3590e+00,  5.6560e-01,\n",
      "        -5.8000e-03,  2.4340e-01,  5.8350e-01,  4.9940e-01,  7.0440e-01,\n",
      "         3.7640e-01,  8.6820e-01,  4.3260e-01,  1.3192e+00,  1.6350e-01,\n",
      "         7.0540e-01,  3.3170e-01,  5.1300e-01,  3.7930e-01,  5.4520e-01,\n",
      "         3.6143e+00,  1.0664e+00,  2.1400e-01,  6.3580e-01,  6.0190e-01,\n",
      "         6.4590e-01,  1.3097e+00,  8.7030e-01,  8.2770e-01,  1.0870e+00,\n",
      "         5.5000e-01,  5.3990e-01,  6.7300e-01,  7.7500e-01,  8.6630e-01,\n",
      "         1.2804e+00,  8.6600e-01,  4.2930e-01,  6.3150e-01,  6.1140e-01,\n",
      "         1.3715e+00,  8.9770e-01,  5.3910e-01,  1.0836e+00,  1.6474e+00,\n",
      "         9.2220e-01,  3.1840e-01,  1.2139e+00,  1.4340e-01,  4.7380e-01,\n",
      "         9.3190e-01,  2.2230e-01,  5.1850e-01,  1.1229e+00,  8.4590e-01,\n",
      "         2.7230e-01,  1.0038e+00,  1.2028e+00,  9.0330e-01,  1.7496e+00,\n",
      "         5.7800e-01,  1.3442e+00,  5.5150e-01,  6.7790e-01,  3.9270e-01,\n",
      "         6.3330e-01,  6.4300e-01,  2.3330e-01,  6.4130e-01,  1.4891e+00,\n",
      "         6.5100e-01,  2.2840e-01,  6.3570e-01,  5.6030e-01,  3.2150e-01,\n",
      "         3.0830e-01,  5.1210e-01,  1.2528e+00,  5.0880e-01,  8.4710e-01,\n",
      "         1.1307e+00,  1.3013e+00,  6.2290e-01,  5.2470e-01,  1.4531e+00,\n",
      "         1.0899e+00,  7.1310e-01,  8.5420e-01,  1.2743e+00,  3.9480e-01,\n",
      "         6.6530e-01,  9.6970e-01,  8.5890e-01,  6.9200e-01,  1.0742e+00,\n",
      "         1.4584e+00,  1.5508e+00,  1.3830e-01,  7.5110e-01,  5.3660e-01,\n",
      "         2.9590e-01,  8.6660e-01,  1.4031e+00,  7.5680e-01,  4.3160e-01,\n",
      "         2.2010e-01,  7.0970e-01,  4.4650e-01,  9.9630e-01,  7.3380e-01,\n",
      "         6.1810e-01,  9.8990e-01,  1.5590e+00,  1.3873e+00,  6.0350e-01,\n",
      "         8.1940e-01,  2.7270e-01,  1.1337e+00,  4.8490e-01,  6.9900e-01,\n",
      "         5.9840e-01,  8.1590e-01,  1.4160e-01,  2.4092e+00,  2.3780e-01,\n",
      "         6.2030e-01,  2.5340e-01,  1.6471e+00,  5.3950e-01,  1.2181e+00,\n",
      "         6.0350e-01,  1.8740e-01,  7.6120e-01,  1.0659e+00,  3.8270e-01,\n",
      "         9.9310e-01,  3.8150e-01,  6.4150e-01,  9.3110e-01,  1.5740e+00,\n",
      "         5.7630e-01,  1.1436e+00,  9.4910e-01,  8.1500e-01, -1.7470e-01,\n",
      "         9.5370e-01,  4.2570e-01,  1.1953e+00,  8.7200e-01,  8.4810e-01,\n",
      "         8.3080e-01,  7.1350e-01,  3.9320e-01,  6.3530e-01,  6.2060e-01,\n",
      "         6.8100e-01,  1.9751e+00,  5.7660e-01,  6.6820e-01,  1.8100e-01,\n",
      "         6.7140e-01,  1.5885e+00,  2.7170e-01,  6.8910e-01,  7.7990e-01,\n",
      "         7.4120e-01,  1.4500e-01,  1.1448e+00,  4.7790e-01,  6.9920e-01,\n",
      "         3.2830e-01,  1.1759e+00,  1.0687e+00,  1.1957e+00,  8.6020e-01,\n",
      "         6.1640e-01,  4.1300e-01,  9.7300e-01,  6.9990e-01,  1.0687e+00,\n",
      "         1.1560e+00,  1.2100e-01,  6.9520e-01,  9.7680e-01,  5.6720e-01,\n",
      "         6.4570e-01,  6.9630e-01,  7.1050e-01,  7.9390e-01,  6.2780e-01,\n",
      "         1.1559e+00,  9.7250e-01,  4.5580e-01,  4.1900e-01,  1.8321e+00,\n",
      "         6.4560e-01,  5.1800e-02,  1.6600e-01,  2.7310e-01,  1.1585e+00,\n",
      "         1.0253e+00,  8.5330e-01,  3.3990e-01,  8.3300e-01,  7.1500e-01,\n",
      "         1.8750e+00,  6.6250e-01,  3.9100e-01,  5.3420e-01,  6.4140e-01,\n",
      "         8.6550e-01,  4.8620e-01,  3.0770e-01,  1.3838e+00,  1.1221e+00,\n",
      "         1.3159e+00,  8.9050e-01,  1.0965e+00,  1.0704e+00,  7.4230e-01,\n",
      "         7.6700e-01,  7.1930e-01, -6.5500e-02,  8.3100e-01,  7.0400e-01,\n",
      "         1.0975e+00,  8.1440e-01,  9.1680e-01,  6.8700e-01,  1.6570e-01,\n",
      "         1.7470e-01,  8.0260e-01,  7.0440e-01,  8.5760e-01,  5.5750e-01,\n",
      "         1.4062e+00,  5.5600e-01,  9.2000e-01,  1.1865e+00,  5.2820e-01,\n",
      "         1.8248e+00,  5.4480e-01,  7.3350e-01,  1.0468e+00,  1.0477e+00,\n",
      "         6.8950e-01,  7.0020e-01,  6.3180e-01,  2.9220e-01,  2.5910e-01,\n",
      "         1.3051e+00,  8.7490e-01,  8.0710e-01,  6.2300e-01,  1.0703e+00,\n",
      "         4.1460e-01,  1.1524e+00,  4.3450e-01,  2.8540e-01,  6.1800e-01,\n",
      "         8.1970e-01,  7.2940e-01,  8.0740e-01,  7.4890e-01,  1.6533e+00,\n",
      "         3.5690e-01,  4.4390e-01,  1.5923e+00,  9.0950e-01,  9.5780e-01,\n",
      "         1.1466e+00,  5.4810e-01,  1.1514e+00,  7.9410e-01,  7.0160e-01,\n",
      "         9.2200e-01,  8.2530e-01,  2.7160e-01,  2.6700e-02,  7.0190e-01,\n",
      "         8.6150e-01,  7.1830e-01,  2.2520e-01,  1.0569e+00,  9.9950e-01,\n",
      "         7.9840e-01,  1.4070e-01,  7.4090e-01,  1.5269e+00,  9.9650e-01,\n",
      "         8.7370e-01,  1.0507e+00,  8.4620e-01,  1.4545e+00,  6.3080e-01,\n",
      "         8.9010e-01,  3.6050e-01,  8.0350e-01,  1.3248e+00,  8.1110e-01,\n",
      "         5.6040e-01,  1.0886e+00,  1.3076e+00,  1.0324e+00,  1.0590e-01,\n",
      "         2.0940e-01,  8.6100e-01,  6.2970e-01,  6.7380e-01,  1.0455e+00,\n",
      "         4.6450e-01,  8.3530e-01,  9.5070e-01,  9.9790e-01,  7.1930e-01,\n",
      "         1.2253e+00,  5.3340e-01,  5.9400e-01,  4.2100e-01,  9.2980e-01,\n",
      "         9.6980e-01,  1.4480e-01,  7.4210e-01,  7.1460e-01,  4.6620e-01,\n",
      "         4.4950e-01,  7.8550e-01,  1.2925e+00,  7.5680e-01,  6.8360e-01,\n",
      "         1.1098e+00,  7.9910e-01,  1.2061e+00,  1.5661e+00,  2.2960e-01,\n",
      "         5.2050e-01,  6.2550e-01,  1.2680e+00,  1.5373e+00,  1.7187e+00,\n",
      "         6.7540e-01,  6.2200e-01,  3.1120e-01,  5.0480e-01,  1.5040e-01,\n",
      "         7.6940e-01,  7.1180e-01,  7.3030e-01,  1.1977e+00,  9.5470e-01,\n",
      "         4.7320e-01,  4.5360e-01,  9.0270e-01,  5.3210e-01,  9.6270e-01,\n",
      "         3.7130e-01,  7.7050e-01,  1.2198e+00,  1.1379e+00,  3.6890e-01,\n",
      "         5.3690e-01,  8.5400e-01,  4.8670e-01,  1.4040e-01,  5.5300e-01,\n",
      "         1.5231e+00,  1.0400e-02,  7.2140e-01,  6.2250e-01,  1.0633e+00,\n",
      "         1.4978e+00,  1.0015e+00,  1.1182e+00,  1.0008e+00,  4.8470e-01,\n",
      "         6.6180e-01,  6.5540e-01,  1.7633e+00,  1.0590e-01,  4.2790e-01,\n",
      "         6.9240e-01, -1.1130e-01,  1.7800e-01,  3.0590e-01,  4.1500e-01,\n",
      "         8.1430e-01,  4.8830e-01,  1.3793e+00,  5.8220e-01,  4.7540e-01,\n",
      "         1.0413e+00,  9.4820e-01,  2.8600e-01,  9.3540e-01,  6.4360e-01,\n",
      "         8.2760e-01,  5.4750e-01,  8.0460e-01,  3.5980e-01,  1.0369e+00,\n",
      "         8.6900e-02,  9.7960e-01,  6.0540e-01,  9.8000e-01,  7.4180e-01,\n",
      "         6.6720e-01,  9.8690e-01,  1.4423e+00,  3.6900e-01,  4.9620e-01,\n",
      "         6.4770e-01,  1.9800e-02,  3.0400e-01,  1.5629e+00,  5.2820e-01,\n",
      "         7.7850e-01,  7.6740e-01, -3.9000e-03,  3.8890e-01,  1.4713e+00,\n",
      "         9.6180e-01,  3.8890e-01,  1.2396e+00,  7.0760e-01,  1.3701e+00,\n",
      "         8.8900e-01,  9.3820e-01,  9.3840e-01,  9.4210e-01,  5.0020e-01,\n",
      "         5.5710e-01,  9.6390e-01,  4.8860e-01,  9.0140e-01,  3.5760e-01,\n",
      "         1.3469e+00,  1.1961e+00,  7.3410e-01,  6.2310e-01,  1.3784e+00,\n",
      "         8.0210e-01,  6.8910e-01,  5.1380e-01,  1.0533e+00,  5.3360e-01,\n",
      "         1.5865e+00,  2.9300e-01,  6.6340e-01,  1.5214e+00,  9.0030e-01,\n",
      "         5.4550e-01,  5.3930e-01,  6.3120e-01,  6.6110e-01,  8.5470e-01,\n",
      "         8.3070e-01,  8.8900e-01,  3.9790e-01,  6.0550e-01,  9.6070e-01,\n",
      "         1.3365e+00,  7.7520e-01,  1.5944e+00,  6.9990e-01,  8.2110e-01,\n",
      "         8.5750e-01,  8.6310e-01,  7.8310e-01,  8.4360e-01,  1.3987e+00,\n",
      "         6.4450e-01,  1.0840e+00,  1.6253e+00,  9.3290e-01,  4.9710e-01,\n",
      "         9.9170e-01,  7.3940e-01,  1.0766e+00,  1.1510e+00,  9.9130e-01,\n",
      "         1.0169e+00,  8.4760e-01,  1.3273e+00,  8.7500e-01,  1.0657e+00,\n",
      "         1.8121e+00,  3.5060e-01,  4.6410e-01,  5.1300e-01,  1.3417e+00,\n",
      "        -2.1980e-01,  8.3770e-01,  9.2640e-01,  9.8930e-01,  8.9560e-01,\n",
      "         5.7610e-01,  1.0705e+00,  9.7770e-01,  1.2526e+00,  8.8260e-01,\n",
      "         7.5240e-01,  9.1750e-01,  1.5386e+00,  8.0460e-01,  9.3030e-01,\n",
      "         5.0170e-01,  2.7200e-01,  1.2742e+00,  5.7910e-01,  1.2734e+00,\n",
      "         8.6040e-01,  7.5390e-01,  8.0150e-01,  4.2680e-01,  4.1210e-01,\n",
      "         9.2990e-01,  9.2860e-01,  1.4042e+00,  1.4830e-01,  8.2330e-01,\n",
      "         1.0099e+00,  1.2872e+00,  5.3840e-01,  8.6450e-01,  9.8770e-01,\n",
      "         8.9850e-01,  8.0100e-01,  8.6950e-01,  7.3960e-01,  1.0210e+00,\n",
      "         1.2048e+00,  2.0730e-01, -4.5700e-02,  7.2640e-01,  8.4280e-01,\n",
      "         2.9290e-01,  6.3940e-01,  6.7270e-01,  8.6790e-01,  4.9030e-01,\n",
      "         1.2041e+00,  8.5310e-01,  6.5390e-01,  4.5810e-01,  6.0330e-01,\n",
      "         4.3870e-01,  1.0907e+00,  2.7000e-01,  9.3790e-01,  8.9610e-01,\n",
      "         9.3400e-01,  3.5810e-01,  8.4580e-01,  6.8970e-01,  4.4740e-01,\n",
      "         6.3300e-01,  9.3390e-01,  3.0710e-01,  3.3720e-01, -8.2700e-02,\n",
      "         9.7320e-01,  5.9530e-01,  4.6980e-01,  5.4730e-01,  7.6960e-01,\n",
      "         1.5002e+00,  5.3390e-01,  1.2635e+00,  1.3200e-01,  4.7440e-01,\n",
      "         5.1840e-01,  2.0977e+00,  1.0565e+00,  7.8470e-01,  1.0683e+00,\n",
      "         5.7630e-01,  5.3020e-01,  1.1435e+00, -1.2170e-01,  1.4355e+00,\n",
      "         5.0720e-01,  9.5840e-01,  9.3410e-01]], dtype=float32)}) \n",
      "\n",
      "('f0e6446d-7f66-5425-b4bf-2820c05a069a', {'Sector': '325412', 'Description': 'ISTA Pharmaceuticals, Inc., a multi-specialty pharmaceutical company, engages in developing, marketing, and selling products in the United States (U.S.) and Puerto Rico. The company manufactures its finished good products through third-party contracts, and in-licenses or acquires new products and technologies to add to its internal development activities from time to tim', 'Description Emb': array([[1.373 , 1.5219, 1.3254, 0.6086, 1.9921, 0.7464, 0.7638, 1.3783,\n",
      "        1.2702, 0.8968, 0.8759, 0.8495, 0.8911, 1.4986, 0.839 , 1.5279,\n",
      "        1.7995, 1.2219, 0.9469, 1.3137, 0.4533, 0.8106, 1.4784, 1.5138,\n",
      "        1.2044, 1.0085, 0.947 , 1.018 , 0.4847, 0.8575, 1.8053, 0.9907,\n",
      "        0.7146, 0.8747, 1.8254, 1.3333, 0.5012, 0.9404, 0.9211, 0.9963,\n",
      "        0.579 , 0.856 , 0.5485, 1.0957, 0.8442, 0.7065, 1.8146, 1.2032,\n",
      "        1.6261, 1.7848, 1.0675, 0.9906, 0.7567, 0.6356, 1.0917, 1.3376,\n",
      "        0.8092, 0.6317, 0.6616, 0.5772, 1.5079, 0.8179, 1.679 , 0.7695,\n",
      "        2.2672, 1.4811, 1.2742, 0.8878, 0.529 , 1.0483, 0.8857, 1.335 ,\n",
      "        1.0473, 1.1395, 0.9026, 1.2378, 0.6654, 0.9742, 0.7921, 1.3259,\n",
      "        1.3657, 1.5424, 1.0227, 1.1461, 0.7169, 1.2821, 1.5653, 1.4144,\n",
      "        0.8122, 1.6085, 0.8743, 1.0417, 1.6044, 0.8017, 0.9481, 1.1912,\n",
      "        1.3349, 0.7216, 0.7477, 1.7915, 0.9344, 1.0898, 0.986 , 0.9079,\n",
      "        0.3837, 0.7593, 0.8746, 1.0018, 0.6426, 1.4347, 0.8238, 0.7417,\n",
      "        1.0974, 0.8136, 0.5517, 1.054 , 1.1022, 1.3927, 1.0101, 0.5978,\n",
      "        1.635 , 1.1636, 1.0831, 1.8946, 0.7304, 0.7786, 1.3772, 0.8504,\n",
      "        0.8669, 0.455 , 1.2679, 1.0131, 0.8616, 1.0199, 0.5647, 1.1302,\n",
      "        1.106 , 0.6988, 1.36  , 0.9661, 1.3938, 0.9343, 1.3288, 0.47  ,\n",
      "        1.0639, 0.5281, 1.0462, 0.9071, 0.6899, 1.5114, 1.2279, 1.2003,\n",
      "        0.5738, 0.9624, 1.0068, 1.1884, 1.3294, 1.2456, 1.1146, 0.6063,\n",
      "        1.1791, 1.3856, 0.8041, 1.3323, 0.5704, 1.1275, 1.1812, 1.5768,\n",
      "        0.6232, 0.8798, 1.035 , 0.6578, 1.8536, 0.6109, 1.7598, 1.1224,\n",
      "        1.2222, 0.8634, 1.3032, 1.0346, 0.4321, 1.65  , 1.2507, 1.002 ,\n",
      "        0.8986, 0.8226, 2.1164, 0.5177, 1.024 , 0.9204, 0.5601, 0.7173,\n",
      "        0.539 , 0.6777, 0.9379, 0.7409, 0.4893, 1.1424, 0.8056, 0.6063,\n",
      "        1.0796, 1.076 , 0.9647, 0.8046, 0.7096, 3.9095, 1.1353, 0.4214,\n",
      "        0.9062, 0.8108, 0.6991, 1.3229, 1.0003, 1.0728, 1.2244, 0.7006,\n",
      "        0.7745, 1.5675, 0.891 , 0.8186, 1.1692, 0.7614, 0.7445, 1.4049,\n",
      "        0.5599, 2.6879, 0.8114, 0.3498, 1.3881, 1.447 , 0.9284, 1.0832,\n",
      "        1.9314, 0.7228, 0.9939, 0.9016, 0.4156, 1.1639, 1.2131, 0.8302,\n",
      "        0.8371, 1.6947, 1.0657, 0.6378, 1.7871, 0.9349, 1.0067, 0.8388,\n",
      "        0.8067, 0.5539, 0.8228, 0.6277, 0.5474, 0.684 , 1.6166, 0.9519,\n",
      "        0.8624, 0.8234, 0.9149, 0.8235, 1.125 , 0.8432, 1.7621, 1.5532,\n",
      "        0.6397, 0.5974, 1.6776, 1.4552, 0.876 , 1.8548, 1.1615, 1.1471,\n",
      "        0.8577, 1.1515, 0.6154, 1.0068, 1.0947, 1.2663, 0.8146, 0.7049,\n",
      "        1.0134, 1.2334, 0.7091, 0.7607, 0.7361, 0.6985, 1.2037, 2.0653,\n",
      "        0.9572, 1.0615, 0.7827, 1.0134, 0.4429, 1.2671, 1.0094, 0.5376,\n",
      "        0.654 , 1.0358, 1.2298, 0.7896, 1.1142, 0.4776, 1.5299, 1.0921,\n",
      "        1.719 , 1.2921, 0.8152, 0.4884, 2.2044, 1.4445, 0.8976, 1.0608,\n",
      "        1.6781, 1.1191, 0.8848, 0.9044, 0.5184, 1.2923, 1.5988, 0.4794,\n",
      "        1.5218, 1.2343, 1.1471, 0.9133, 1.1029, 0.6911, 1.131 , 1.3242,\n",
      "        0.9465, 0.2819, 0.8772, 1.0906, 1.7093, 1.5411, 1.235 , 0.9122,\n",
      "        0.9084, 0.9639, 0.6392, 0.501 , 0.9139, 1.5751, 1.0229, 1.0948,\n",
      "        0.7111, 0.6396, 1.687 , 0.3968, 1.1292, 0.8529, 1.2575, 0.5984,\n",
      "        1.9626, 0.7561, 1.3912, 1.1269, 1.6174, 1.5349, 1.3937, 1.1304,\n",
      "        0.8231, 1.1211, 1.1609, 0.7022, 1.1718, 1.5455, 0.8522, 1.3398,\n",
      "        1.4225, 1.0661, 0.6381, 0.6997, 1.9911, 1.0647, 0.6927, 0.671 ,\n",
      "        1.3166, 0.8315, 1.1133, 1.5705, 0.8181, 0.5677, 1.0731, 0.7137,\n",
      "        1.3346, 0.6627, 1.345 , 0.8085, 0.6824, 0.817 , 1.7387, 1.512 ,\n",
      "        0.8996, 0.5661, 0.5615, 1.9081, 0.5634, 0.5913, 1.4021, 1.7772,\n",
      "        1.3447, 0.9067, 1.5913, 0.6921, 0.9651, 1.4996, 0.9363, 1.0236,\n",
      "        1.0644, 1.1554, 1.5434, 0.4818, 0.7579, 0.9655, 0.7019, 0.6756,\n",
      "        1.5239, 0.956 , 0.7231, 0.8142, 1.2857, 0.457 , 1.1836, 1.1407,\n",
      "        0.9354, 1.7316, 0.7278, 0.6402, 0.8122, 1.4514, 0.8114, 1.2966,\n",
      "        1.7301, 0.7593, 0.5248, 1.7278, 0.7523, 0.7735, 1.3812, 1.1374,\n",
      "        1.4642, 1.1624, 1.2947, 1.3678, 0.9021, 0.8446, 0.8487, 1.6938,\n",
      "        1.2692, 1.6272, 0.8882, 0.7941, 1.2374, 1.3999, 1.5988, 1.2983,\n",
      "        0.5693, 1.135 , 1.1288, 0.7265, 0.7198, 0.9336, 1.3027, 0.9076,\n",
      "        1.5164, 0.6103, 1.0396, 0.5873, 0.902 , 1.189 , 1.2683, 0.8286,\n",
      "        0.731 , 1.757 , 0.5817, 0.8153, 1.1748, 1.1421, 1.2262, 0.7017,\n",
      "        1.183 , 0.9283, 0.6168, 1.46  , 0.8674, 0.9832, 1.021 , 1.4692,\n",
      "        1.1926, 0.4302, 0.3905, 1.0825, 1.2208, 0.8137, 1.4237, 0.6532,\n",
      "        0.8758, 0.7462, 1.3171, 1.3784, 0.518 , 1.1979, 0.814 , 0.5362,\n",
      "        1.1882, 1.4983, 0.9491, 1.4423, 0.8342, 1.0162, 0.7868, 1.2839,\n",
      "        1.119 , 0.6072, 1.1811, 1.4544, 1.3431, 1.0015, 0.6849, 0.3637,\n",
      "        1.4181, 1.0506, 0.9755, 1.4959, 0.9308, 1.0087, 0.4599, 1.1862,\n",
      "        0.5483, 0.4665, 1.4056, 0.8362, 0.8315, 1.4558, 0.7835, 1.0542,\n",
      "        0.6874, 0.7984, 1.0669, 0.7578, 0.7984, 0.9459, 1.3959, 1.3999,\n",
      "        0.7124, 0.6469, 1.3586, 0.7444, 0.7561, 0.7766, 0.7408, 1.0633,\n",
      "        1.1573, 1.349 , 1.2926, 1.4005, 0.8817, 1.4773, 0.4437, 0.7156,\n",
      "        0.8097, 1.1521, 1.2344, 0.4969, 0.5908, 0.9388, 0.4077, 0.6105,\n",
      "        0.5694, 1.769 , 0.9586, 0.9061, 1.4387, 0.7844, 0.5166, 1.593 ,\n",
      "        1.1847, 0.7268, 0.9935, 0.7902, 0.972 , 1.0231, 0.7198, 1.1035,\n",
      "        1.6292, 0.5199, 1.114 , 1.1314, 1.9636, 0.9183, 0.992 , 0.9805,\n",
      "        2.0117, 0.8595, 1.0203, 1.1927, 0.9056, 0.5995, 1.2658, 1.1666,\n",
      "        1.1947, 1.5691, 0.4686, 0.8962, 1.6666, 0.7548, 1.1142, 0.9805,\n",
      "        0.9923, 1.3729, 1.3424, 0.8166, 0.7324, 1.3826, 1.1327, 0.4135,\n",
      "        0.8874, 1.5144, 1.4144, 0.4567, 1.6416, 1.1983, 1.0244, 1.0458,\n",
      "        0.8849, 0.9694, 1.149 , 0.7802, 1.1417, 0.8839, 1.5853, 0.6183,\n",
      "        1.499 , 1.5437, 0.9538, 0.7713, 0.8014, 1.1068, 0.8056, 0.7528,\n",
      "        0.8008, 1.019 , 1.3315, 0.8599, 0.6527, 1.4132, 1.3816, 1.0163,\n",
      "        0.9131, 1.0131, 1.2697, 1.303 , 1.2184, 0.9932, 1.2998, 1.1501,\n",
      "        1.3366, 1.5285, 1.7758, 1.0022, 1.3912, 1.7077, 1.7912, 1.6545,\n",
      "        1.7841, 1.184 , 0.9057, 1.4853, 0.9045, 1.1997, 2.0525, 0.4426,\n",
      "        0.8825, 1.0103, 1.5199, 0.4812, 0.954 , 1.114 , 1.0868, 1.1768,\n",
      "        0.6049, 1.2361, 1.0477, 1.3766, 1.1112, 1.2461, 1.358 , 1.4732,\n",
      "        0.8102, 0.6477, 0.4276, 0.823 , 1.8327, 0.6976, 0.829 , 1.0372,\n",
      "        1.0708, 0.9848, 1.1033, 0.6389, 1.3978, 0.8432, 1.6979, 1.0939,\n",
      "        1.7842, 0.923 , 1.1465, 0.7188, 1.1876, 1.3631, 0.59  , 1.2271,\n",
      "        1.3962, 0.8284, 1.125 , 1.3868, 0.8478, 0.6587, 1.3991, 1.2239,\n",
      "        0.4409, 1.1214, 0.8882, 1.2352, 0.9651, 1.3257, 1.2097, 1.1289,\n",
      "        0.7086, 0.7878, 0.7658, 1.0588, 0.5248, 0.8365, 1.6008, 1.3682,\n",
      "        1.0663, 0.9186, 1.0577, 0.931 , 0.9397, 1.6744, 0.6882, 0.6839,\n",
      "        0.6058, 1.2698, 0.7836, 1.0202, 0.9769, 1.895 , 1.5072, 0.8484,\n",
      "        1.8192, 0.417 , 0.5272, 0.6209, 1.7457, 0.8156, 1.0279, 0.9593,\n",
      "        1.0468, 0.9714, 0.9363, 0.3395, 1.7602, 0.7531, 1.0002, 1.0046]],\n",
      "      dtype=float32)}) \n",
      "\n",
      "('f3fc74d2-a9ae-5cfd-8a41-a510086c1781', {'Sector': '325412', 'Description': 'Nektar Therapeutics operates as a research-based biopharmaceutical company. The company focuses on discovering and developing innovative medicines in areas of high unmet medical nee', 'Description Emb': array([[0.6979, 1.0116, 0.8939, 0.5742, 1.5059, 0.7463, 0.545 , 1.0351,\n",
      "        1.0806, 0.7752, 0.6482, 0.3593, 0.9191, 1.5509, 0.5172, 1.8671,\n",
      "        0.5472, 1.2588, 1.1527, 0.7793, 1.5536, 0.3315, 0.9593, 1.7058,\n",
      "        1.0334, 0.6928, 1.0285, 0.9506, 0.5168, 0.5734, 1.2864, 1.1021,\n",
      "        0.6494, 0.829 , 1.0828, 1.0206, 0.3056, 1.1954, 0.4072, 0.9381,\n",
      "        0.4133, 0.5697, 0.268 , 1.2181, 0.6654, 0.7577, 1.1356, 0.4346,\n",
      "        1.0671, 0.7757, 0.5227, 0.6761, 0.5662, 0.3699, 0.8451, 1.3319,\n",
      "        0.6822, 0.4768, 0.4623, 0.5524, 1.2934, 0.658 , 0.5853, 0.8967,\n",
      "        1.4491, 1.0842, 0.9253, 1.1854, 0.1923, 1.3518, 0.7096, 1.0091,\n",
      "        0.8588, 0.8016, 0.865 , 0.484 , 0.9122, 1.2592, 0.8393, 0.8426,\n",
      "        1.0638, 1.3896, 0.967 , 1.5206, 0.5043, 1.3691, 1.0249, 1.2466,\n",
      "        0.4556, 1.4831, 1.0277, 0.7703, 0.895 , 0.6397, 1.5975, 0.6673,\n",
      "        0.783 , 0.6231, 1.2452, 1.747 , 1.0983, 0.5194, 0.5691, 0.7306,\n",
      "        0.5541, 0.6328, 0.7568, 0.6798, 0.854 , 1.3332, 1.3818, 0.6922,\n",
      "        0.586 , 0.4058, 0.4103, 1.325 , 0.6508, 1.2025, 0.85  , 0.5196,\n",
      "        0.8714, 1.47  , 0.6319, 2.0982, 0.3772, 0.7113, 0.8904, 1.0155,\n",
      "        0.9452, 1.1333, 1.3599, 1.3029, 0.6842, 0.5687, 0.613 , 0.8586,\n",
      "        1.2516, 0.354 , 1.1245, 0.9583, 0.7779, 0.6449, 1.6394, 1.3228,\n",
      "        0.9188, 0.4394, 0.8534, 1.3804, 1.0056, 1.3428, 1.378 , 1.2338,\n",
      "        0.4043, 0.5665, 0.8657, 0.6023, 0.6553, 1.1562, 0.9458, 0.6852,\n",
      "        0.966 , 0.9138, 0.437 , 0.937 , 0.3256, 1.0959, 1.3502, 1.1404,\n",
      "        0.6115, 1.0406, 0.8788, 0.3251, 1.3462, 0.4293, 1.0227, 0.93  ,\n",
      "        0.8336, 0.4186, 0.68  , 1.0307, 0.1629, 1.7637, 1.2541, 0.9019,\n",
      "        1.0965, 0.7207, 2.0356, 0.7877, 1.1949, 1.1057, 0.1684, 0.8752,\n",
      "        0.701 , 0.796 , 1.0407, 0.6016, 0.7295, 1.5134, 0.5673, 0.3034,\n",
      "        0.6058, 1.0311, 1.125 , 0.6027, 0.7593, 3.5651, 1.1061, 0.5558,\n",
      "        0.5854, 0.7297, 0.8443, 0.9461, 1.6539, 0.6201, 1.4006, 0.4659,\n",
      "        0.5385, 1.107 , 0.5451, 0.8048, 0.9655, 1.2001, 0.5432, 1.198 ,\n",
      "        0.5746, 1.9207, 0.7281, 0.6152, 1.1804, 1.5983, 0.4181, 0.8768,\n",
      "        1.6261, 0.3551, 0.6154, 0.7604, 0.2651, 0.5812, 1.0829, 0.6928,\n",
      "        0.4721, 0.8616, 0.657 , 0.7661, 2.1037, 0.4647, 0.8643, 0.9676,\n",
      "        0.608 , 0.2533, 0.2668, 0.7361, 0.6233, 0.6762, 1.2915, 0.5839,\n",
      "        0.6603, 0.901 , 0.6057, 0.8206, 0.9743, 0.9694, 1.7546, 0.7463,\n",
      "        0.8223, 0.4954, 1.1979, 0.718 , 1.064 , 0.7296, 1.0981, 0.7947,\n",
      "        0.9142, 1.245 , 0.5277, 0.3691, 0.773 , 1.3381, 0.6575, 0.8983,\n",
      "        0.992 , 1.1654, 0.6927, 0.6496, 0.7034, 0.5943, 1.2067, 2.0783,\n",
      "        0.5051, 1.0466, 0.5713, 0.6685, 0.3818, 1.2424, 0.6075, 0.5581,\n",
      "        0.7394, 1.6809, 0.9537, 0.5938, 0.7616, 0.4171, 1.4047, 1.0702,\n",
      "        0.5884, 0.601 , 1.3089, 0.4796, 1.7931, 0.5326, 0.7497, 0.7279,\n",
      "        1.9557, 0.5283, 1.1247, 1.0483, 0.588 , 0.7899, 1.0196, 0.6002,\n",
      "        0.8409, 0.4309, 1.2338, 1.0581, 1.1935, 0.5113, 0.9402, 1.0346,\n",
      "        0.7264, 0.2902, 0.7111, 0.9517, 0.9   , 0.4719, 0.3885, 1.0182,\n",
      "        0.3908, 0.2623, 1.2452, 0.3001, 0.7785, 1.2974, 1.1309, 0.6855,\n",
      "        0.6628, 0.6181, 0.8595, 0.4658, 1.0098, 0.7763, 0.5306, 0.3944,\n",
      "        1.5715, 0.4765, 1.1131, 1.4223, 0.6464, 1.0166, 0.8845, 0.3247,\n",
      "        0.919 , 0.517 , 1.5449, 0.4401, 0.8588, 1.4819, 0.3323, 1.242 ,\n",
      "        0.7452, 0.8237, 0.5133, 0.4531, 0.8958, 0.7782, 0.5414, 0.443 ,\n",
      "        0.7298, 0.6794, 0.9956, 1.2354, 0.8793, 0.2322, 1.196 , 0.6394,\n",
      "        1.2081, 1.0498, 0.6811, 0.9254, 0.3704, 0.944 , 1.2199, 0.6321,\n",
      "        0.8635, 0.4139, 0.5478, 1.052 , 0.3383, 1.06  , 1.4235, 1.1367,\n",
      "        1.1702, 1.0364, 1.0925, 0.5793, 0.6271, 1.4446, 1.0757, 0.7967,\n",
      "        1.0823, 0.6945, 1.5247, 0.5309, 0.8759, 0.6576, 0.458 , 0.2622,\n",
      "        1.3272, 0.8308, 0.879 , 0.8734, 1.3504, 0.6057, 1.4129, 0.8343,\n",
      "        0.9683, 1.2343, 0.9117, 0.6593, 0.5739, 1.3278, 0.6316, 0.6195,\n",
      "        1.0626, 0.7768, 0.2956, 0.9029, 0.3759, 0.7284, 1.3253, 1.2392,\n",
      "        0.6501, 1.1524, 1.2139, 0.6287, 0.9132, 0.7386, 0.6126, 1.114 ,\n",
      "        0.9991, 0.9433, 1.0185, 1.1708, 1.073 , 0.6661, 1.1077, 1.0703,\n",
      "        0.9579, 0.9675, 0.9081, 0.9233, 0.4812, 1.5925, 1.3381, 1.4   ,\n",
      "        1.0894, 0.6918, 0.7496, 0.563 , 1.01  , 1.254 , 1.4043, 0.7104,\n",
      "        0.5401, 1.4438, 0.6813, 0.9599, 1.0373, 0.5761, 0.9546, 0.535 ,\n",
      "        0.7481, 0.7168, 0.584 , 1.2205, 0.947 , 0.7606, 0.739 , 1.5136,\n",
      "        0.6762, 0.7938, 0.4624, 0.9228, 1.3496, 0.9665, 1.1915, 0.3477,\n",
      "        0.9568, 1.036 , 1.0155, 0.9508, 0.9082, 1.0242, 0.7573, 0.5453,\n",
      "        1.0147, 1.0619, 0.8663, 0.782 , 1.3568, 0.8857, 0.8489, 0.7397,\n",
      "        0.713 , 0.5859, 0.676 , 1.0697, 0.6528, 1.5056, 0.5944, 0.2991,\n",
      "        0.5891, 0.8725, 0.8383, 1.1014, 1.0109, 0.5349, 0.5536, 1.068 ,\n",
      "        0.8972, 0.782 , 1.3869, 0.662 , 0.9071, 1.0319, 1.0048, 1.2944,\n",
      "        0.3924, 0.8045, 0.3601, 1.0298, 0.8122, 0.49  , 1.426 , 0.9635,\n",
      "        1.0492, 0.8547, 0.9102, 0.5324, 0.316 , 0.6777, 0.5296, 0.5301,\n",
      "        0.9039, 1.0409, 1.7982, 1.4373, 0.4692, 1.5118, 0.411 , 0.4853,\n",
      "        0.6909, 1.0026, 0.7435, 0.4303, 0.7056, 0.5825, 0.3673, 0.5171,\n",
      "        0.5991, 1.0876, 0.7348, 0.7883, 1.4144, 0.795 , 0.4619, 1.0355,\n",
      "        1.1061, 0.6609, 0.7991, 0.9119, 0.739 , 0.91  , 0.8133, 1.1281,\n",
      "        1.2115, 0.7063, 1.3231, 0.7798, 2.037 , 0.8443, 1.2224, 0.7991,\n",
      "        1.924 , 0.4633, 0.5417, 1.2845, 0.8129, 0.3419, 1.1773, 0.6629,\n",
      "        0.4562, 1.2408, 0.6426, 1.4084, 1.5269, 1.2145, 1.4085, 1.2553,\n",
      "        1.1303, 0.9753, 1.3055, 0.6909, 0.831 , 1.499 , 1.1942, 0.6775,\n",
      "        1.0393, 0.8533, 0.6004, 0.7996, 1.7402, 1.391 , 0.6197, 0.7396,\n",
      "        0.302 , 0.5508, 0.8413, 0.7191, 0.5335, 1.0831, 0.9443, 0.3238,\n",
      "        1.0742, 1.4889, 0.5263, 0.8162, 0.9024, 1.7099, 0.6855, 1.1025,\n",
      "        0.5543, 1.4222, 0.7661, 0.9271, 0.7263, 1.6344, 0.6029, 0.5818,\n",
      "        1.4821, 0.6755, 1.0817, 1.6732, 1.4858, 0.9805, 1.1465, 0.9439,\n",
      "        1.0707, 2.1131, 0.8394, 0.8633, 1.2008, 1.671 , 1.135 , 1.1208,\n",
      "        1.2637, 1.0299, 0.6347, 1.7684, 0.8836, 0.8483, 1.7131, 0.5055,\n",
      "        0.7401, 0.9247, 1.675 , 0.2936, 0.7941, 0.87  , 0.8514, 1.0455,\n",
      "        0.4364, 1.0526, 0.6627, 1.7952, 0.8372, 0.5436, 0.5787, 0.8766,\n",
      "        0.5689, 0.5168, 0.2412, 0.2808, 1.1327, 1.1262, 0.8903, 0.5351,\n",
      "        0.7904, 0.5398, 0.9898, 0.9576, 1.5443, 0.7743, 0.5129, 0.9761,\n",
      "        1.6948, 0.9224, 0.8889, 0.8758, 1.3008, 0.9841, 0.5969, 0.9724,\n",
      "        0.7288, 0.9031, 1.0177, 1.0646, 0.5536, 0.3111, 1.033 , 1.1612,\n",
      "        0.4756, 1.0523, 0.7014, 1.0006, 0.7856, 1.0732, 1.0944, 1.1262,\n",
      "        0.8145, 0.7861, 1.0069, 0.8673, 0.2378, 0.9271, 1.1889, 1.0073,\n",
      "        0.6576, 0.876 , 0.6319, 0.8856, 0.7496, 0.9631, 0.5684, 0.5355,\n",
      "        0.4637, 0.8723, 0.5286, 0.4927, 0.8556, 1.1977, 2.0006, 0.729 ,\n",
      "        1.2927, 0.8224, 0.349 , 0.5586, 0.8578, 0.983 , 0.5164, 0.8462,\n",
      "        1.1678, 0.8024, 1.0374, 0.5218, 0.9767, 0.43  , 1.2641, 0.9383]],\n",
      "      dtype=float32)}) \n",
      "\n",
      "('f43889b6-3216-52a1-8870-0dd5cef34d5b', {'Sector': '511210', 'Description': 'Incentra Solutions, Inc. provides IT solutions and services to enterprises and managed service providers in North America and Europe. The company\\x92s solution includes managed services, professional services, hardware and software products, maintenance contracts, recurring managed services and capital financing solution', 'Description Emb': array([[1.0231, 1.053 , 1.2382, 1.0495, 1.7857, 0.6054, 0.552 , 1.351 ,\n",
      "        1.5849, 0.52  , 0.5108, 0.4415, 1.0229, 1.4673, 0.717 , 1.0857,\n",
      "        0.7677, 0.9792, 0.9042, 1.5712, 0.7613, 0.378 , 1.4032, 1.6979,\n",
      "        1.0763, 1.1449, 0.6501, 0.6499, 0.6273, 0.8059, 1.4264, 0.8558,\n",
      "        1.2939, 0.7765, 1.0168, 0.7965, 0.3916, 0.6542, 0.2156, 0.8143,\n",
      "        0.4415, 0.597 , 1.504 , 0.9451, 1.1395, 0.589 , 0.8717, 0.5807,\n",
      "        1.4427, 0.8229, 0.2719, 1.2507, 0.8957, 0.363 , 1.4637, 1.5143,\n",
      "        1.0645, 0.6358, 0.523 , 0.3392, 0.979 , 0.6641, 0.715 , 0.8998,\n",
      "        1.6784, 0.7161, 1.4408, 1.2495, 0.1402, 0.8278, 1.5415, 0.9222,\n",
      "        0.7462, 0.7971, 0.5742, 0.7479, 0.387 , 1.37  , 0.746 , 1.0732,\n",
      "        1.1081, 1.5094, 0.863 , 1.6214, 1.0007, 1.4853, 1.0707, 0.8716,\n",
      "        0.4884, 1.4568, 1.07  , 0.8174, 0.9764, 0.8825, 0.8926, 0.6342,\n",
      "        0.8973, 0.5239, 0.9575, 1.335 , 1.0051, 0.9307, 0.7109, 0.555 ,\n",
      "        0.6392, 0.7196, 0.9381, 0.8731, 0.5909, 1.0581, 0.7744, 0.628 ,\n",
      "        1.041 , 0.4586, 0.6927, 1.1608, 0.7658, 1.2064, 1.0533, 0.5811,\n",
      "        1.1934, 1.4663, 1.1415, 1.7675, 0.5392, 1.1771, 0.8796, 0.8337,\n",
      "        0.9654, 0.8747, 1.3382, 0.9874, 0.6536, 0.8691, 0.6483, 1.2088,\n",
      "        0.7164, 0.2143, 1.5963, 0.9984, 0.9231, 1.0368, 1.3225, 1.1265,\n",
      "        0.7261, 0.4844, 1.4391, 0.6104, 0.6113, 1.3223, 1.1284, 1.0663,\n",
      "        0.4424, 1.0748, 0.7192, 0.5698, 1.0714, 0.8172, 0.953 , 0.5044,\n",
      "        1.6146, 1.0507, 1.3843, 1.8706, 0.426 , 0.3954, 0.7923, 1.3505,\n",
      "        0.4549, 0.8468, 0.9661, 0.9157, 1.5692, 0.6028, 1.922 , 1.1939,\n",
      "        0.9019, 1.0889, 1.4981, 0.8933, 0.0102, 1.1616, 1.4019, 0.687 ,\n",
      "        1.5505, 0.8968, 2.2615, 0.7602, 1.3371, 0.7434, 0.4579, 0.2088,\n",
      "        1.0295, 0.6278, 0.7842, 0.363 , 0.8639, 0.5468, 0.8224, 0.6771,\n",
      "        0.9745, 1.0259, 0.7799, 0.8284, 0.4949, 3.6535, 0.732 , 0.3394,\n",
      "        0.8809, 0.9775, 0.7741, 0.8141, 0.7973, 1.0136, 1.3826, 0.2652,\n",
      "        0.4381, 0.9534, 1.2937, 0.8523, 1.7716, 0.5388, 0.6103, 1.2969,\n",
      "        0.6739, 1.6257, 1.1143, 0.3628, 1.3438, 1.6255, 0.8128, 0.8463,\n",
      "        1.3567, 0.3238, 0.9561, 0.7866, 0.6514, 0.4789, 0.9635, 0.7788,\n",
      "        0.4644, 1.6817, 1.2686, 0.5688, 1.5024, 0.5142, 0.8352, 0.7604,\n",
      "        0.5701, 0.7957, 0.5732, 0.8665, 0.6114, 0.1746, 1.5656, 0.7597,\n",
      "        0.6545, 1.1524, 0.8251, 0.732 , 0.9187, 1.1167, 2.3291, 1.0605,\n",
      "        0.9468, 0.5148, 0.9832, 0.9112, 0.9127, 0.9685, 1.1352, 0.641 ,\n",
      "        0.6149, 1.1628, 0.8324, 0.6   , 1.0714, 1.1551, 0.5637, 0.9462,\n",
      "        1.3524, 1.3527, 0.5383, 0.9652, 1.0013, 1.0144, 1.0713, 1.0978,\n",
      "        0.6675, 1.2387, 0.7507, 0.9063, 0.2131, 0.8916, 1.2242, 0.8924,\n",
      "        0.7097, 1.4535, 1.317 , 1.1184, 0.6912, 0.4403, 1.3446, 0.8232,\n",
      "        0.904 , 0.5491, 0.8532, 0.3122, 3.6135, 0.6704, 0.9792, 0.511 ,\n",
      "        1.3556, 0.3761, 1.2522, 0.6251, 0.5961, 1.1149, 1.7848, 0.515 ,\n",
      "        0.8323, 0.5519, 0.4436, 1.2569, 1.2356, 0.7839, 0.904 , 1.2274,\n",
      "        0.6743, 0.1166, 1.0962, 0.719 , 1.3464, 1.5073, 0.9822, 1.1703,\n",
      "        0.4504, 0.5872, 1.3988, 0.5666, 0.6744, 1.5742, 0.5699, 0.407 ,\n",
      "        0.7649, 0.6892, 1.4239, 0.2876, 1.2551, 0.7929, 0.7719, 1.1975,\n",
      "        1.6197, 0.3576, 1.1377, 1.1866, 0.9436, 1.5158, 1.3586, 0.8285,\n",
      "        0.6864, 0.4239, 0.6933, 0.647 , 0.947 , 1.3325, 0.396 , 1.213 ,\n",
      "        1.3355, 0.8584, 0.6627, 0.4958, 0.9164, 0.5519, 0.5977, 0.5683,\n",
      "        0.7045, 0.3742, 1.0235, 1.3638, 1.1687, 0.1582, 1.1812, 0.3632,\n",
      "        1.3057, 0.7384, 0.8765, 0.4379, 0.7361, 0.9485, 1.4074, 0.9268,\n",
      "        0.9596, 0.8444, 0.7167, 0.7594, 0.5915, 0.5648, 1.5429, 1.5493,\n",
      "        1.3484, 0.9254, 1.322 , 1.2397, 1.1587, 0.8577, 0.851 , 0.1526,\n",
      "        0.8324, 1.1641, 1.5474, 0.3458, 0.49  , 1.0557, 0.5839, 0.3837,\n",
      "        1.0121, 0.8444, 1.0636, 0.7962, 1.1265, 0.862 , 1.3267, 1.0411,\n",
      "        0.9778, 1.3565, 0.8247, 0.6559, 0.9415, 1.3692, 1.1367, 0.7667,\n",
      "        0.6841, 1.0422, 1.1159, 2.1327, 1.3274, 0.3638, 1.4858, 0.7207,\n",
      "        1.2319, 1.2259, 0.7082, 0.6019, 0.8809, 0.7705, 0.5431, 0.5482,\n",
      "        0.7658, 1.9347, 0.7714, 1.0475, 1.5805, 1.0925, 0.7223, 1.3245,\n",
      "        0.8647, 1.1425, 0.8079, 0.3657, 1.0631, 1.0932, 0.8676, 0.9268,\n",
      "        1.1806, 0.8606, 0.5954, 0.4174, 1.2098, 0.8637, 1.3591, 0.7282,\n",
      "        1.1324, 1.7157, 0.1844, 1.1456, 0.9881, 0.7432, 1.1592, 0.8611,\n",
      "        1.3587, 0.6672, 0.6679, 1.145 , 1.1815, 0.8301, 0.4084, 1.3517,\n",
      "        0.9529, 0.6044, 0.3266, 0.9844, 0.9932, 1.4625, 1.1028, 0.6083,\n",
      "        0.644 , 1.1371, 1.1896, 1.4215, 1.0505, 1.1468, 0.7634, 0.3542,\n",
      "        1.366 , 1.2069, 0.6195, 0.8348, 0.7388, 1.2364, 0.419 , 0.85  ,\n",
      "        0.7898, 0.5047, 1.8881, 0.8022, 1.0684, 1.4748, 0.4335, 0.7319,\n",
      "        0.9101, 0.5002, 0.7811, 1.1682, 1.3575, 0.5145, 0.6279, 1.1961,\n",
      "        0.504 , 0.5046, 0.6148, 0.8408, 0.5809, 1.0179, 0.6502, 0.9441,\n",
      "        0.479 , 0.7387, 0.4519, 0.8653, 0.8787, 0.546 , 1.1401, 1.1907,\n",
      "        0.7292, 0.8599, 1.184 , 0.7355, 0.164 , 0.6882, 0.6717, 0.666 ,\n",
      "        0.8815, 0.8591, 1.1013, 1.3935, 1.0351, 1.7518, 0.9194, 0.5966,\n",
      "        0.7679, 0.964 , 1.1641, 0.4784, 0.5941, 0.7729, 0.1565, 0.4168,\n",
      "        0.5168, 0.8525, 0.9075, 0.9629, 1.3384, 0.9645, 0.8131, 0.9127,\n",
      "        0.9195, 0.6956, 1.4813, 0.8851, 1.2628, 0.98  , 0.9838, 0.6297,\n",
      "        1.4763, 0.3845, 1.046 , 0.8518, 2.0959, 1.1008, 0.9608, 0.8108,\n",
      "        1.8717, 0.3224, 1.1523, 1.02  , 1.1265, 0.668 , 0.8025, 0.9139,\n",
      "        0.8149, 1.8782, 0.1761, 0.8003, 1.6015, 0.7244, 1.1303, 0.8692,\n",
      "        0.8334, 1.2921, 1.2526, 0.8724, 0.3582, 0.8986, 0.7744, 0.2228,\n",
      "        0.5312, 0.8712, 0.6196, 0.4188, 1.4182, 1.7829, 0.5183, 0.785 ,\n",
      "        0.581 , 0.6798, 1.1963, 0.914 , 0.8779, 1.1092, 0.9329, 1.249 ,\n",
      "        1.0479, 1.5435, 0.5467, 0.789 , 1.3134, 0.8023, 0.8308, 0.8668,\n",
      "        0.991 , 1.6106, 0.9195, 1.2267, 0.7453, 1.751 , 0.5122, 0.8193,\n",
      "        1.4707, 0.8979, 1.6043, 1.4295, 0.919 , 0.9984, 1.472 , 0.9219,\n",
      "        1.3662, 1.067 , 1.1391, 1.2669, 1.1437, 1.698 , 1.1516, 0.9982,\n",
      "        1.3018, 1.1238, 0.9688, 1.5121, 1.1503, 0.6449, 2.7462, 0.418 ,\n",
      "        0.56  , 0.8252, 1.084 , 0.2487, 0.6226, 0.7566, 1.5782, 1.2541,\n",
      "        0.292 , 0.7412, 1.3361, 1.3078, 0.9581, 0.6141, 1.2785, 1.226 ,\n",
      "        0.5527, 0.7915, 1.1661, 0.4341, 1.4992, 1.4625, 0.4559, 0.9948,\n",
      "        1.3682, 0.5007, 0.7344, 0.3662, 1.5757, 0.866 , 1.6558, 0.6829,\n",
      "        1.4027, 0.8447, 1.6793, 0.8613, 1.6975, 1.3169, 0.8308, 0.9049,\n",
      "        0.7926, 1.1421, 1.2458, 1.353 , 0.4093, 0.5659, 1.1065, 1.3718,\n",
      "        0.5604, 1.0727, 1.0084, 0.8307, 0.9744, 1.1237, 0.9214, 0.8164,\n",
      "        0.4981, 0.4356, 1.4583, 0.6085, 0.9393, 1.2008, 1.1788, 1.0011,\n",
      "        1.0165, 1.1249, 0.5504, 0.8282, 0.8744, 1.6299, 0.336 , 0.6825,\n",
      "        0.2979, 1.044 , 0.7908, 0.8123, 0.6174, 1.0532, 0.9652, 1.4855,\n",
      "        1.1772, 0.7495, 0.3624, 0.4755, 1.8682, 1.0916, 0.9606, 0.6389,\n",
      "        1.0605, 0.7015, 0.8376, 0.7722, 0.812 , 0.6853, 1.1544, 0.9863]],\n",
      "      dtype=float32)}) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedder = er.EmbeddingRetriever()\n",
    "\n",
    "with open('Description.csv','r') as file:\n",
    "    next(file)\n",
    "    file = csv.reader(file, delimiter = ';')\n",
    "\n",
    "    EN_Des = {}\n",
    "    for row in file:\n",
    "        try:\n",
    "            ID,sector,description = row[0].split(\",\", 2)\n",
    "            ID = ID[1:-1]\n",
    "            if ID in Data_Set_N.keys():\n",
    "                sector = sector[1:-1]\n",
    "                if description == '\"\\n':\n",
    "                    description = \" \"\n",
    "\n",
    "                EN_Des[ID] = {}\n",
    "                EN_Des[ID].update({'Sector' : sector, 'Description' : description[1:-2]})\n",
    "                EN_Des[ID].update({'Description Emb' : np.round(embedder.get_bert_embedding(description[1:-2]),4)})\n",
    "\n",
    "\n",
    "        except ValueError:\n",
    "            continue\n",
    "        \n",
    "n = 0\n",
    "for i in EN_Des.items():\n",
    "    print(i,\"\\n\")\n",
    "    n += 1\n",
    "    if n == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "912c7b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756\n"
     ]
    }
   ],
   "source": [
    "print(len(EN_Des))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0879d3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.83098 0.20714 0.87999 ... 0.10433 0.28876 0.4544 ]\n",
      " [0.76542 0.25424 0.65867 ... 0.37093 0.29731 0.21371]\n",
      " [0.58009 0.27592 0.57657 ... 0.39631 0.2833  0.4119 ]]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "Text_Embedding = embedder.get_bert_embeddings(['Hello World','Hi','Hello'])\n",
    "print(np.round(Text_Embedding,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b4344bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e809884c-8121-52f8-806c-6d1b86adae2e', {'Sector': '\"511210\"', 'Revenue': [0.5, 0.5, 0.5, 0.5, 0.5], 'EBITDA': [0, 0, 0, 0, 0], 'NDE': [1.0, 0, 0, 0.2, 0], 'TDE': [0.53, 0.88, 0.81, 0, 0.29], 'Description Embedding': array([[ 5.7600e-01,  7.7410e-01,  8.9120e-01,  7.5210e-01,  1.1872e+00,\n",
      "         6.7610e-01,  2.7440e-01,  1.0162e+00,  8.5640e-01,  6.3140e-01,\n",
      "         4.3880e-01,  2.7280e-01,  7.2570e-01,  1.8619e+00,  5.3800e-01,\n",
      "         1.2024e+00,  2.8100e-01,  9.9480e-01,  5.8390e-01,  6.9910e-01,\n",
      "         4.3010e-01,  4.7800e-02,  7.4360e-01,  1.4881e+00,  9.0410e-01,\n",
      "         6.2760e-01,  2.2730e-01,  3.8470e-01,  2.5190e-01,  8.9040e-01,\n",
      "         7.7160e-01,  9.8240e-01,  9.1970e-01,  4.1410e-01,  5.5550e-01,\n",
      "         3.5180e-01,  3.0050e-01,  5.6390e-01,  6.5250e-01,  7.1340e-01,\n",
      "         4.6900e-02,  3.4330e-01,  1.1250e+00,  4.7230e-01,  9.4290e-01,\n",
      "         6.1250e-01,  1.1234e+00,  1.2333e+00,  7.3890e-01,  9.0870e-01,\n",
      "         7.8400e-02,  1.4007e+00,  1.0414e+00,  4.7210e-01,  9.2410e-01,\n",
      "         9.7060e-01,  9.2720e-01,  2.3050e-01,  1.4018e+00,  5.3680e-01,\n",
      "         4.3480e-01,  4.9840e-01,  6.7420e-01,  6.8190e-01,  8.4540e-01,\n",
      "         2.9210e-01,  8.2160e-01,  1.1460e+00,  8.7200e-02,  6.0140e-01,\n",
      "         7.9110e-01,  9.6290e-01,  4.5830e-01,  8.3580e-01,  3.8720e-01,\n",
      "         7.5700e-01,  3.1710e-01,  1.3720e+00,  8.7490e-01,  9.2380e-01,\n",
      "         7.2900e-01,  1.3657e+00,  1.0210e+00,  8.7610e-01,  7.7440e-01,\n",
      "         1.1606e+00,  5.3500e-01,  9.9230e-01,  4.3440e-01,  1.1933e+00,\n",
      "         6.9220e-01,  6.6470e-01,  1.1385e+00,  8.9600e-01,  8.7220e-01,\n",
      "         6.4970e-01,  4.2570e-01,  3.2480e-01,  6.8420e-01,  1.4858e+00,\n",
      "         9.2330e-01,  4.1750e-01,  7.8430e-01,  6.1980e-01,  9.1870e-01,\n",
      "         9.1590e-01,  1.0318e+00,  9.8090e-01,  7.7070e-01,  8.4010e-01,\n",
      "         1.2458e+00,  4.1190e-01,  6.5020e-01,  5.4460e-01,  4.4960e-01,\n",
      "         1.1755e+00,  5.3760e-01,  5.9770e-01,  1.1536e+00,  2.9140e-01,\n",
      "         8.1900e-01,  1.1130e+00,  6.3040e-01,  1.7215e+00,  3.7850e-01,\n",
      "         9.7720e-01,  6.9410e-01,  9.0290e-01,  1.1049e+00,  5.8460e-01,\n",
      "         8.3780e-01,  7.4560e-01,  5.6700e-01,  6.2250e-01,  5.3380e-01,\n",
      "         9.3950e-01,  6.7550e-01,  2.9540e-01,  1.2010e+00,  5.2570e-01,\n",
      "         8.8280e-01,  7.2350e-01,  1.0542e+00,  7.3840e-01,  8.1700e-01,\n",
      "         4.7920e-01,  1.0844e+00,  5.5200e-01,  4.9560e-01,  1.1218e+00,\n",
      "         1.1230e+00,  1.1643e+00,  4.0830e-01,  2.2570e-01,  6.6510e-01,\n",
      "         4.9910e-01,  5.8850e-01,  1.1248e+00,  1.4639e+00,  5.1860e-01,\n",
      "         1.2309e+00,  1.2955e+00,  1.6110e-01,  1.0656e+00,  4.1910e-01,\n",
      "         8.1720e-01,  9.3310e-01,  1.0777e+00,  5.9950e-01,  1.1889e+00,\n",
      "         9.7320e-01,  5.3500e-01,  9.6830e-01,  4.8740e-01,  1.9520e+00,\n",
      "         7.8390e-01,  7.2300e-01,  3.1510e-01,  4.4460e-01,  9.1090e-01,\n",
      "        -1.4400e-02,  1.1405e+00,  1.1290e+00,  7.6150e-01,  9.1070e-01,\n",
      "         5.4940e-01,  1.0711e+00,  3.1070e-01,  1.1933e+00,  5.5920e-01,\n",
      "        -1.0900e-02,  5.6600e-02,  6.0800e-01,  6.1850e-01,  5.8500e-01,\n",
      "         4.2840e-01,  6.2900e-01,  7.2070e-01,  9.4650e-01,  6.7540e-01,\n",
      "         7.7120e-01,  5.7010e-01,  8.3410e-01,  7.3860e-01,  4.1770e-01,\n",
      "         3.3158e+00,  1.0464e+00,  2.0730e-01,  2.0310e-01,  4.3520e-01,\n",
      "         5.7950e-01,  6.9700e-01,  6.6780e-01,  9.6780e-01,  1.2082e+00,\n",
      "         6.1770e-01,  3.4950e-01,  1.0570e+00,  6.5830e-01,  6.7420e-01,\n",
      "         1.3153e+00,  7.3630e-01,  3.1010e-01,  9.5450e-01,  2.4220e-01,\n",
      "         1.5653e+00,  6.3850e-01,  6.4320e-01,  1.0112e+00,  7.6960e-01,\n",
      "         7.5980e-01,  5.2460e-01,  1.2976e+00,  4.9200e-01,  6.1030e-01,\n",
      "         7.2920e-01,  2.6950e-01,  4.0140e-01,  1.2653e+00,  4.7830e-01,\n",
      "         2.8000e-03,  1.1209e+00,  1.1700e+00,  3.1010e-01,  1.7323e+00,\n",
      "         5.1880e-01,  1.2695e+00,  1.0123e+00,  9.0950e-01,  3.4530e-01,\n",
      "         7.8430e-01,  8.1440e-01,  5.9650e-01,  3.6210e-01,  1.3203e+00,\n",
      "         5.6290e-01,  6.0680e-01,  9.4110e-01,  3.7220e-01,  1.1645e+00,\n",
      "         8.8780e-01,  5.7770e-01,  1.3478e+00,  9.7650e-01,  8.8320e-01,\n",
      "         1.1280e-01,  9.5050e-01,  1.0435e+00,  6.6050e-01,  7.9780e-01,\n",
      "         9.2910e-01,  4.2970e-01,  9.0940e-01,  1.2130e+00,  4.0940e-01,\n",
      "         4.4760e-01,  9.6560e-01,  8.3330e-01,  7.2510e-01,  4.5560e-01,\n",
      "         1.0214e+00,  6.9840e-01,  9.0230e-01,  6.2630e-01,  1.0190e+00,\n",
      "         4.9170e-01,  1.0699e+00,  7.9950e-01,  7.0030e-01,  5.1890e-01,\n",
      "         4.4050e-01,  9.4870e-01,  2.6390e-01,  8.7030e-01,  1.0820e+00,\n",
      "         8.4030e-01,  7.0340e-01,  1.1469e+00,  6.1240e-01,  5.1810e-01,\n",
      "         7.3720e-01,  3.7560e-01,  1.2739e+00,  5.2860e-01,  7.5730e-01,\n",
      "         3.8030e-01,  1.2256e+00,  4.7200e-01,  2.2187e+00,  3.4130e-01,\n",
      "         1.0785e+00,  3.1040e-01,  1.7760e+00,  4.9960e-01,  1.0523e+00,\n",
      "         5.1570e-01,  5.2920e-01,  1.1292e+00,  1.0032e+00,  5.1970e-01,\n",
      "         9.4860e-01,  6.3230e-01,  5.0020e-01,  1.5533e+00,  1.0085e+00,\n",
      "         2.3360e-01,  8.1710e-01,  7.7710e-01,  3.4850e-01,  4.9970e-01,\n",
      "         6.9240e-01,  9.1590e-01,  1.0334e+00,  6.8830e-01,  5.7950e-01,\n",
      "         1.2182e+00,  5.3780e-01,  9.8600e-01,  6.7600e-01,  2.7790e-01,\n",
      "         4.2460e-01,  1.3832e+00,  2.7840e-01,  2.0770e-01,  5.3380e-01,\n",
      "         8.5540e-01,  1.4485e+00,  2.9080e-01,  1.2281e+00,  1.0099e+00,\n",
      "         5.2040e-01,  7.8100e-01,  1.3666e+00,  2.8720e-01,  9.0520e-01,\n",
      "         7.1300e-01,  7.1780e-01,  5.7290e-01,  1.1051e+00,  4.7660e-01,\n",
      "         1.0445e+00,  5.0370e-01,  5.6260e-01,  2.7160e-01,  9.7900e-01,\n",
      "         9.3370e-01, -2.1180e-01,  9.0490e-01,  9.2150e-01,  5.4940e-01,\n",
      "         5.2150e-01,  5.4680e-01,  1.1553e+00,  6.2190e-01,  4.6240e-01,\n",
      "         8.2640e-01,  1.5880e-01,  4.3320e-01,  5.5540e-01,  1.4502e+00,\n",
      "         9.2320e-01,  1.2010e-01,  6.5630e-01,  1.1200e-01,  8.3240e-01,\n",
      "         4.6360e-01,  7.4280e-01,  4.3110e-01,  4.2110e-01,  1.0456e+00,\n",
      "         9.4090e-01,  1.0344e+00,  8.9430e-01,  6.2300e-01,  5.5440e-01,\n",
      "         5.8320e-01,  5.5400e-01,  5.6450e-01,  9.5790e-01,  1.3408e+00,\n",
      "         1.2076e+00,  5.9230e-01,  1.1063e+00,  7.5740e-01,  8.6460e-01,\n",
      "         9.1860e-01,  5.0820e-01,  4.1310e-01,  3.8450e-01,  6.9680e-01,\n",
      "         1.0080e+00,  4.1120e-01,  1.8770e-01,  1.1646e+00,  4.6550e-01,\n",
      "         3.1110e-01,  1.0025e+00,  7.1630e-01,  5.7210e-01,  6.6520e-01,\n",
      "         9.1890e-01,  7.5160e-01,  3.3760e-01,  8.5520e-01,  4.6540e-01,\n",
      "         1.3953e+00,  6.9500e-01,  4.7210e-01,  5.4020e-01,  9.5140e-01,\n",
      "         6.7350e-01,  8.4340e-01,  8.0580e-01,  2.3180e-01,  2.1380e-01,\n",
      "         1.3490e+00,  8.6330e-01,  3.5910e-01,  2.8330e-01,  1.0511e+00,\n",
      "         7.1400e-01,  9.2300e-01,  6.7950e-01,  4.0390e-01,  7.9200e-01,\n",
      "         7.0470e-01,  5.6330e-01,  6.6510e-01,  9.0130e-01,  1.5015e+00,\n",
      "         1.0804e+00,  6.5480e-01,  1.1089e+00,  1.0939e+00,  9.0820e-01,\n",
      "         1.1964e+00,  4.2350e-01,  9.4450e-01,  6.2310e-01,  5.5880e-01,\n",
      "         7.2210e-01,  9.5490e-01,  1.0526e+00,  7.0280e-01,  9.5210e-01,\n",
      "         7.1920e-01,  7.0870e-01,  2.4800e-01,  5.6740e-01,  9.8270e-01,\n",
      "         8.2170e-01,  3.6460e-01,  6.4430e-01,  1.3420e+00,  4.7440e-01,\n",
      "         8.1930e-01,  1.2287e+00,  1.0178e+00,  1.3057e+00,  1.2918e+00,\n",
      "         5.9900e-01,  2.1250e-01,  5.8650e-01,  1.0946e+00,  9.0320e-01,\n",
      "         3.1690e-01,  5.0270e-01,  1.0152e+00,  6.8550e-01,  5.8480e-01,\n",
      "         2.1110e-01,  7.6070e-01,  1.2690e+00,  9.7130e-01,  6.4010e-01,\n",
      "         5.3320e-01,  6.4290e-01,  1.0131e+00,  1.2535e+00,  7.1470e-01,\n",
      "         2.5200e-02,  7.6080e-01,  6.6500e-01,  2.4600e-01,  9.3510e-01,\n",
      "         1.0354e+00,  5.0190e-01,  7.5520e-01,  7.1370e-01,  6.9830e-01,\n",
      "         2.4150e-01,  7.2770e-01,  3.7760e-01,  2.5250e-01,  1.1719e+00,\n",
      "         9.6100e-01,  1.1740e+00,  8.5170e-01,  8.3210e-01,  2.0300e-01,\n",
      "         1.1035e+00,  6.4990e-01,  9.0050e-01,  9.7050e-01,  1.0788e+00,\n",
      "         4.9230e-01,  4.0490e-01,  7.1130e-01,  1.7480e-01,  4.6590e-01,\n",
      "         8.1730e-01,  1.2643e+00,  4.4080e-01,  8.2270e-01,  6.3350e-01,\n",
      "         1.0466e+00,  5.8480e-01,  3.4320e-01,  8.5010e-01,  7.6360e-01,\n",
      "         4.5630e-01,  6.2720e-01,  1.0761e+00,  1.0029e+00,  3.3410e-01,\n",
      "         5.6610e-01,  9.6620e-01,  7.8480e-01,  3.5470e-01,  2.5560e-01,\n",
      "         1.0210e-01,  4.5030e-01,  8.1040e-01,  9.1460e-01,  6.1620e-01,\n",
      "         1.4756e+00,  8.9080e-01,  1.2784e+00,  1.4688e+00,  7.6650e-01,\n",
      "         6.0900e-01,  8.4830e-01,  9.8240e-01,  1.9830e-01,  2.9830e-01,\n",
      "         7.4720e-01, -1.0610e-01,  5.0880e-01,  3.7700e-01,  8.3920e-01,\n",
      "         7.4390e-01,  7.5260e-01,  7.1500e-01,  5.2620e-01,  7.2530e-01,\n",
      "         6.7260e-01,  5.3020e-01,  7.0020e-01,  6.4890e-01,  6.0890e-01,\n",
      "         1.0717e+00,  7.0130e-01,  4.8130e-01,  6.1100e-01,  1.5806e+00,\n",
      "         4.7080e-01,  8.1690e-01,  7.9200e-01,  1.4291e+00,  6.4050e-01,\n",
      "         1.0094e+00,  1.1721e+00,  1.1607e+00,  4.5010e-01,  1.1807e+00,\n",
      "         7.3770e-01,  7.3410e-01,  6.6320e-01,  8.1150e-01,  1.0932e+00,\n",
      "         3.5180e-01,  9.4070e-01,  8.1260e-01,  8.3070e-01,  1.3986e+00,\n",
      "         8.0480e-01,  1.2073e+00,  1.1972e+00,  6.9200e-01,  1.2515e+00,\n",
      "         9.5400e-01,  7.4240e-01,  5.3750e-01,  1.2665e+00,  6.9840e-01,\n",
      "         2.5890e-01,  3.3490e-01,  1.2690e-01,  5.2180e-01,  4.4610e-01,\n",
      "         1.5166e+00,  1.3049e+00,  3.1930e-01,  9.5590e-01,  5.7550e-01,\n",
      "         7.6090e-01,  5.5750e-01,  9.5440e-01,  3.4820e-01,  7.6800e-01,\n",
      "         1.2224e+00,  2.3340e-01,  8.4420e-01,  1.4350e+00,  5.5250e-01,\n",
      "         1.1074e+00,  5.4370e-01,  8.6720e-01,  8.3790e-01,  8.6750e-01,\n",
      "         8.3100e-01,  1.2162e+00,  7.4260e-01,  6.3430e-01,  1.0879e+00,\n",
      "         6.9420e-01,  7.6500e-01,  8.4300e-01,  1.0318e+00,  7.7100e-01,\n",
      "         1.0767e+00,  1.1279e+00,  4.9650e-01,  9.1280e-01,  1.1768e+00,\n",
      "         9.4770e-01,  8.5640e-01,  1.2069e+00,  1.0414e+00,  6.9420e-01,\n",
      "         1.1166e+00,  1.0302e+00,  1.1527e+00,  8.1560e-01,  5.8100e-01,\n",
      "         1.0824e+00,  9.3610e-01,  1.5946e+00,  7.7890e-01,  5.1320e-01,\n",
      "         1.8855e+00,  3.0120e-01,  7.4400e-02,  9.0230e-01,  1.7598e+00,\n",
      "         6.8810e-01,  6.7310e-01,  5.2810e-01,  6.5080e-01,  1.2834e+00,\n",
      "         8.1780e-01,  7.8000e-01,  8.4130e-01,  8.8650e-01,  9.2230e-01,\n",
      "         4.8700e-01,  5.4610e-01,  1.1601e+00,  6.5480e-01,  4.8520e-01,\n",
      "         3.6160e-01,  4.3190e-01,  1.3086e+00,  5.2940e-01,  1.1080e-01,\n",
      "         6.4440e-01,  1.4059e+00,  8.2120e-01,  3.6670e-01,  4.0180e-01,\n",
      "         1.4283e+00,  7.2150e-01,  1.1477e+00,  4.5650e-01,  9.4150e-01,\n",
      "         5.6310e-01,  8.6840e-01,  1.1127e+00,  1.2129e+00,  8.7100e-01,\n",
      "         3.9930e-01,  6.9920e-01,  4.6770e-01,  9.9130e-01,  1.2173e+00,\n",
      "         1.1433e+00,  2.7070e-01,  5.1060e-01,  6.6080e-01,  1.0994e+00,\n",
      "         4.4390e-01,  4.0930e-01,  3.3710e-01,  1.0307e+00,  6.5950e-01,\n",
      "         1.0234e+00,  6.6390e-01,  7.5370e-01,  1.1330e-01,  3.0330e-01,\n",
      "         6.6380e-01,  6.6050e-01,  8.4200e-01,  1.2946e+00,  1.2279e+00,\n",
      "         8.3190e-01,  5.5240e-01,  7.7570e-01,  5.9720e-01,  6.5490e-01,\n",
      "         7.8890e-01,  1.1310e+00,  6.6500e-01,  7.1470e-01,  3.1800e-01,\n",
      "         1.4710e+00,  6.6550e-01,  3.8510e-01,  1.0170e+00,  3.5720e-01,\n",
      "         9.0600e-01,  6.8980e-01,  1.1729e+00,  2.8030e-01,  6.2090e-01,\n",
      "         1.5680e-01,  2.2714e+00,  1.1464e+00,  1.4325e+00,  2.4270e-01,\n",
      "         4.1200e-01,  6.3080e-01,  9.5640e-01,  7.2870e-01,  8.2780e-01,\n",
      "         3.5740e-01,  5.7520e-01,  7.4890e-01]], dtype=float32)}) \n",
      "\n",
      "('f09eeed5-ae28-59af-92f8-43b51d392970', {'Sector': '\"2111\"', 'Revenue': [0.6, 0.6, 0.6, 0.6, 0.5], 'EBITDA': [0.5, 0.6, 0.6, 0.5, 0], 'NDE': [0, 0, 0, 0, 0.7], 'TDE': [0, 0, 0.32, 0.43, 0.7], 'Description Embedding': array([[ 5.6460e-01,  6.1530e-01,  5.8600e-01,  6.4610e-01,  1.2041e+00,\n",
      "         4.0600e-01,  1.1300e-02,  9.8770e-01,  8.6360e-01,  8.5760e-01,\n",
      "         3.4970e-01,  4.7230e-01,  7.5610e-01,  1.3572e+00,  1.0499e+00,\n",
      "         1.9999e+00,  3.8580e-01,  6.6860e-01,  6.6180e-01,  8.1970e-01,\n",
      "         4.6060e-01,  5.0480e-01,  1.1044e+00,  1.0924e+00,  9.6170e-01,\n",
      "         1.0186e+00,  5.6730e-01,  1.0911e+00, -8.6600e-02,  6.9460e-01,\n",
      "         1.3459e+00,  9.4790e-01,  6.6940e-01,  8.7670e-01,  1.1967e+00,\n",
      "         5.5700e-01,  3.9650e-01,  4.2650e-01,  4.1440e-01,  5.5750e-01,\n",
      "         2.4010e-01,  7.1170e-01,  7.1890e-01,  6.6020e-01,  7.9680e-01,\n",
      "         4.4280e-01,  8.9830e-01,  3.8700e-01,  3.4410e-01,  5.9570e-01,\n",
      "         7.3300e-01,  8.6750e-01,  3.3600e-01,  7.6900e-01,  9.8760e-01,\n",
      "         1.3978e+00,  1.0346e+00,  4.6480e-01,  6.1280e-01,  4.2350e-01,\n",
      "         8.4110e-01,  5.8770e-01,  8.1460e-01,  3.5670e-01,  1.0677e+00,\n",
      "         3.2740e-01,  1.0807e+00,  7.4230e-01,  4.7950e-01,  8.4550e-01,\n",
      "         5.1470e-01,  8.3720e-01,  8.6940e-01,  7.2910e-01,  1.1474e+00,\n",
      "         9.7390e-01,  1.1637e+00,  6.9320e-01,  5.2900e-01,  1.6124e+00,\n",
      "         1.1977e+00,  1.2037e+00,  5.7610e-01,  9.6850e-01,  3.9800e-01,\n",
      "         6.2530e-01,  7.3940e-01,  6.5750e-01,  8.8600e-01,  1.6134e+00,\n",
      "         4.3790e-01,  5.5710e-01,  8.3160e-01,  6.1660e-01,  5.3540e-01,\n",
      "         3.2520e-01,  1.3440e+00,  3.4360e-01,  8.1590e-01,  1.3776e+00,\n",
      "         7.7720e-01,  1.3168e+00,  4.9070e-01,  7.8700e-01,  3.6000e-03,\n",
      "         7.8880e-01,  5.8910e-01,  8.5060e-01,  8.5820e-01,  4.8550e-01,\n",
      "         1.1921e+00,  7.2910e-01,  9.3080e-01,  3.7630e-01,  3.3160e-01,\n",
      "         8.1940e-01,  3.9160e-01,  7.0040e-01,  1.1901e+00,  2.2340e-01,\n",
      "         1.1414e+00,  4.4400e-01,  3.6640e-01,  1.6199e+00,  7.1470e-01,\n",
      "         6.9930e-01,  9.3240e-01,  7.1520e-01,  8.4000e-01,  3.4010e-01,\n",
      "         1.5603e+00,  1.0055e+00,  7.0130e-01,  6.1410e-01,  3.5580e-01,\n",
      "         1.0252e+00,  8.0790e-01,  3.9440e-01,  1.2574e+00,  4.0220e-01,\n",
      "         6.5350e-01,  6.8030e-01,  5.8440e-01,  4.6850e-01,  8.6480e-01,\n",
      "         1.8970e-01,  6.8860e-01,  5.8740e-01,  6.7410e-01,  1.2512e+00,\n",
      "         9.5720e-01,  7.0870e-01,  3.8300e-02,  6.8290e-01,  4.9930e-01,\n",
      "        -2.3350e-01,  1.1223e+00,  5.8240e-01,  1.2082e+00,  5.4730e-01,\n",
      "         1.0091e+00,  6.4170e-01,  1.1048e+00,  9.4080e-01,  5.0880e-01,\n",
      "         3.9670e-01,  7.6010e-01,  8.2700e-01,  2.0330e-01,  6.3760e-01,\n",
      "         3.0900e-01,  5.9400e-02,  1.0721e+00,  5.4500e-01,  9.9190e-01,\n",
      "         1.0337e+00,  3.7500e-01,  1.0684e+00,  9.0820e-01,  9.6710e-01,\n",
      "        -8.5000e-02,  1.6639e+00,  9.1720e-01,  1.4114e+00,  1.2405e+00,\n",
      "         5.8040e-01,  1.3944e+00,  4.8440e-01,  1.3590e+00,  5.6560e-01,\n",
      "        -5.8000e-03,  2.4340e-01,  5.8350e-01,  4.9940e-01,  7.0440e-01,\n",
      "         3.7640e-01,  8.6820e-01,  4.3260e-01,  1.3192e+00,  1.6350e-01,\n",
      "         7.0540e-01,  3.3170e-01,  5.1300e-01,  3.7930e-01,  5.4520e-01,\n",
      "         3.6143e+00,  1.0664e+00,  2.1400e-01,  6.3580e-01,  6.0190e-01,\n",
      "         6.4590e-01,  1.3097e+00,  8.7030e-01,  8.2770e-01,  1.0870e+00,\n",
      "         5.5000e-01,  5.3990e-01,  6.7300e-01,  7.7500e-01,  8.6630e-01,\n",
      "         1.2804e+00,  8.6600e-01,  4.2930e-01,  6.3150e-01,  6.1140e-01,\n",
      "         1.3715e+00,  8.9770e-01,  5.3910e-01,  1.0836e+00,  1.6474e+00,\n",
      "         9.2220e-01,  3.1840e-01,  1.2139e+00,  1.4340e-01,  4.7380e-01,\n",
      "         9.3190e-01,  2.2230e-01,  5.1850e-01,  1.1229e+00,  8.4590e-01,\n",
      "         2.7230e-01,  1.0038e+00,  1.2028e+00,  9.0330e-01,  1.7496e+00,\n",
      "         5.7800e-01,  1.3442e+00,  5.5150e-01,  6.7790e-01,  3.9270e-01,\n",
      "         6.3330e-01,  6.4300e-01,  2.3330e-01,  6.4130e-01,  1.4891e+00,\n",
      "         6.5100e-01,  2.2840e-01,  6.3570e-01,  5.6030e-01,  3.2150e-01,\n",
      "         3.0830e-01,  5.1210e-01,  1.2528e+00,  5.0880e-01,  8.4710e-01,\n",
      "         1.1307e+00,  1.3013e+00,  6.2290e-01,  5.2470e-01,  1.4531e+00,\n",
      "         1.0899e+00,  7.1310e-01,  8.5420e-01,  1.2743e+00,  3.9480e-01,\n",
      "         6.6530e-01,  9.6970e-01,  8.5890e-01,  6.9200e-01,  1.0742e+00,\n",
      "         1.4584e+00,  1.5508e+00,  1.3830e-01,  7.5110e-01,  5.3660e-01,\n",
      "         2.9590e-01,  8.6660e-01,  1.4031e+00,  7.5680e-01,  4.3160e-01,\n",
      "         2.2010e-01,  7.0970e-01,  4.4650e-01,  9.9630e-01,  7.3380e-01,\n",
      "         6.1810e-01,  9.8990e-01,  1.5590e+00,  1.3873e+00,  6.0350e-01,\n",
      "         8.1940e-01,  2.7270e-01,  1.1337e+00,  4.8490e-01,  6.9900e-01,\n",
      "         5.9840e-01,  8.1590e-01,  1.4160e-01,  2.4092e+00,  2.3780e-01,\n",
      "         6.2030e-01,  2.5340e-01,  1.6471e+00,  5.3950e-01,  1.2181e+00,\n",
      "         6.0350e-01,  1.8740e-01,  7.6120e-01,  1.0659e+00,  3.8270e-01,\n",
      "         9.9310e-01,  3.8150e-01,  6.4150e-01,  9.3110e-01,  1.5740e+00,\n",
      "         5.7630e-01,  1.1436e+00,  9.4910e-01,  8.1500e-01, -1.7470e-01,\n",
      "         9.5370e-01,  4.2570e-01,  1.1953e+00,  8.7200e-01,  8.4810e-01,\n",
      "         8.3080e-01,  7.1350e-01,  3.9320e-01,  6.3530e-01,  6.2060e-01,\n",
      "         6.8100e-01,  1.9751e+00,  5.7660e-01,  6.6820e-01,  1.8100e-01,\n",
      "         6.7140e-01,  1.5885e+00,  2.7170e-01,  6.8910e-01,  7.7990e-01,\n",
      "         7.4120e-01,  1.4500e-01,  1.1448e+00,  4.7790e-01,  6.9920e-01,\n",
      "         3.2830e-01,  1.1759e+00,  1.0687e+00,  1.1957e+00,  8.6020e-01,\n",
      "         6.1640e-01,  4.1300e-01,  9.7300e-01,  6.9990e-01,  1.0687e+00,\n",
      "         1.1560e+00,  1.2100e-01,  6.9520e-01,  9.7680e-01,  5.6720e-01,\n",
      "         6.4570e-01,  6.9630e-01,  7.1050e-01,  7.9390e-01,  6.2780e-01,\n",
      "         1.1559e+00,  9.7250e-01,  4.5580e-01,  4.1900e-01,  1.8321e+00,\n",
      "         6.4560e-01,  5.1800e-02,  1.6600e-01,  2.7310e-01,  1.1585e+00,\n",
      "         1.0253e+00,  8.5330e-01,  3.3990e-01,  8.3300e-01,  7.1500e-01,\n",
      "         1.8750e+00,  6.6250e-01,  3.9100e-01,  5.3420e-01,  6.4140e-01,\n",
      "         8.6550e-01,  4.8620e-01,  3.0770e-01,  1.3838e+00,  1.1221e+00,\n",
      "         1.3159e+00,  8.9050e-01,  1.0965e+00,  1.0704e+00,  7.4230e-01,\n",
      "         7.6700e-01,  7.1930e-01, -6.5500e-02,  8.3100e-01,  7.0400e-01,\n",
      "         1.0975e+00,  8.1440e-01,  9.1680e-01,  6.8700e-01,  1.6570e-01,\n",
      "         1.7470e-01,  8.0260e-01,  7.0440e-01,  8.5760e-01,  5.5750e-01,\n",
      "         1.4062e+00,  5.5600e-01,  9.2000e-01,  1.1865e+00,  5.2820e-01,\n",
      "         1.8248e+00,  5.4480e-01,  7.3350e-01,  1.0468e+00,  1.0477e+00,\n",
      "         6.8950e-01,  7.0020e-01,  6.3180e-01,  2.9220e-01,  2.5910e-01,\n",
      "         1.3051e+00,  8.7490e-01,  8.0710e-01,  6.2300e-01,  1.0703e+00,\n",
      "         4.1460e-01,  1.1524e+00,  4.3450e-01,  2.8540e-01,  6.1800e-01,\n",
      "         8.1970e-01,  7.2940e-01,  8.0740e-01,  7.4890e-01,  1.6533e+00,\n",
      "         3.5690e-01,  4.4390e-01,  1.5923e+00,  9.0950e-01,  9.5780e-01,\n",
      "         1.1466e+00,  5.4810e-01,  1.1514e+00,  7.9410e-01,  7.0160e-01,\n",
      "         9.2200e-01,  8.2530e-01,  2.7160e-01,  2.6700e-02,  7.0190e-01,\n",
      "         8.6150e-01,  7.1830e-01,  2.2520e-01,  1.0569e+00,  9.9950e-01,\n",
      "         7.9840e-01,  1.4070e-01,  7.4090e-01,  1.5269e+00,  9.9650e-01,\n",
      "         8.7370e-01,  1.0507e+00,  8.4620e-01,  1.4545e+00,  6.3080e-01,\n",
      "         8.9010e-01,  3.6050e-01,  8.0350e-01,  1.3248e+00,  8.1110e-01,\n",
      "         5.6040e-01,  1.0886e+00,  1.3076e+00,  1.0324e+00,  1.0590e-01,\n",
      "         2.0940e-01,  8.6100e-01,  6.2970e-01,  6.7380e-01,  1.0455e+00,\n",
      "         4.6450e-01,  8.3530e-01,  9.5070e-01,  9.9790e-01,  7.1930e-01,\n",
      "         1.2253e+00,  5.3340e-01,  5.9400e-01,  4.2100e-01,  9.2980e-01,\n",
      "         9.6980e-01,  1.4480e-01,  7.4210e-01,  7.1460e-01,  4.6620e-01,\n",
      "         4.4950e-01,  7.8550e-01,  1.2925e+00,  7.5680e-01,  6.8360e-01,\n",
      "         1.1098e+00,  7.9910e-01,  1.2061e+00,  1.5661e+00,  2.2960e-01,\n",
      "         5.2050e-01,  6.2550e-01,  1.2680e+00,  1.5373e+00,  1.7187e+00,\n",
      "         6.7540e-01,  6.2200e-01,  3.1120e-01,  5.0480e-01,  1.5040e-01,\n",
      "         7.6940e-01,  7.1180e-01,  7.3030e-01,  1.1977e+00,  9.5470e-01,\n",
      "         4.7320e-01,  4.5360e-01,  9.0270e-01,  5.3210e-01,  9.6270e-01,\n",
      "         3.7130e-01,  7.7050e-01,  1.2198e+00,  1.1379e+00,  3.6890e-01,\n",
      "         5.3690e-01,  8.5400e-01,  4.8670e-01,  1.4040e-01,  5.5300e-01,\n",
      "         1.5231e+00,  1.0400e-02,  7.2140e-01,  6.2250e-01,  1.0633e+00,\n",
      "         1.4978e+00,  1.0015e+00,  1.1182e+00,  1.0008e+00,  4.8470e-01,\n",
      "         6.6180e-01,  6.5540e-01,  1.7633e+00,  1.0590e-01,  4.2790e-01,\n",
      "         6.9240e-01, -1.1130e-01,  1.7800e-01,  3.0590e-01,  4.1500e-01,\n",
      "         8.1430e-01,  4.8830e-01,  1.3793e+00,  5.8220e-01,  4.7540e-01,\n",
      "         1.0413e+00,  9.4820e-01,  2.8600e-01,  9.3540e-01,  6.4360e-01,\n",
      "         8.2760e-01,  5.4750e-01,  8.0460e-01,  3.5980e-01,  1.0369e+00,\n",
      "         8.6900e-02,  9.7960e-01,  6.0540e-01,  9.8000e-01,  7.4180e-01,\n",
      "         6.6720e-01,  9.8690e-01,  1.4423e+00,  3.6900e-01,  4.9620e-01,\n",
      "         6.4770e-01,  1.9800e-02,  3.0400e-01,  1.5629e+00,  5.2820e-01,\n",
      "         7.7850e-01,  7.6740e-01, -3.9000e-03,  3.8890e-01,  1.4713e+00,\n",
      "         9.6180e-01,  3.8890e-01,  1.2396e+00,  7.0760e-01,  1.3701e+00,\n",
      "         8.8900e-01,  9.3820e-01,  9.3840e-01,  9.4210e-01,  5.0020e-01,\n",
      "         5.5710e-01,  9.6390e-01,  4.8860e-01,  9.0140e-01,  3.5760e-01,\n",
      "         1.3469e+00,  1.1961e+00,  7.3410e-01,  6.2310e-01,  1.3784e+00,\n",
      "         8.0210e-01,  6.8910e-01,  5.1380e-01,  1.0533e+00,  5.3360e-01,\n",
      "         1.5865e+00,  2.9300e-01,  6.6340e-01,  1.5214e+00,  9.0030e-01,\n",
      "         5.4550e-01,  5.3930e-01,  6.3120e-01,  6.6110e-01,  8.5470e-01,\n",
      "         8.3070e-01,  8.8900e-01,  3.9790e-01,  6.0550e-01,  9.6070e-01,\n",
      "         1.3365e+00,  7.7520e-01,  1.5944e+00,  6.9990e-01,  8.2110e-01,\n",
      "         8.5750e-01,  8.6310e-01,  7.8310e-01,  8.4360e-01,  1.3987e+00,\n",
      "         6.4450e-01,  1.0840e+00,  1.6253e+00,  9.3290e-01,  4.9710e-01,\n",
      "         9.9170e-01,  7.3940e-01,  1.0766e+00,  1.1510e+00,  9.9130e-01,\n",
      "         1.0169e+00,  8.4760e-01,  1.3273e+00,  8.7500e-01,  1.0657e+00,\n",
      "         1.8121e+00,  3.5060e-01,  4.6410e-01,  5.1300e-01,  1.3417e+00,\n",
      "        -2.1980e-01,  8.3770e-01,  9.2640e-01,  9.8930e-01,  8.9560e-01,\n",
      "         5.7610e-01,  1.0705e+00,  9.7770e-01,  1.2526e+00,  8.8260e-01,\n",
      "         7.5240e-01,  9.1750e-01,  1.5386e+00,  8.0460e-01,  9.3030e-01,\n",
      "         5.0170e-01,  2.7200e-01,  1.2742e+00,  5.7910e-01,  1.2734e+00,\n",
      "         8.6040e-01,  7.5390e-01,  8.0150e-01,  4.2680e-01,  4.1210e-01,\n",
      "         9.2990e-01,  9.2860e-01,  1.4042e+00,  1.4830e-01,  8.2330e-01,\n",
      "         1.0099e+00,  1.2872e+00,  5.3840e-01,  8.6450e-01,  9.8770e-01,\n",
      "         8.9850e-01,  8.0100e-01,  8.6950e-01,  7.3960e-01,  1.0210e+00,\n",
      "         1.2048e+00,  2.0730e-01, -4.5700e-02,  7.2640e-01,  8.4280e-01,\n",
      "         2.9290e-01,  6.3940e-01,  6.7270e-01,  8.6790e-01,  4.9030e-01,\n",
      "         1.2041e+00,  8.5310e-01,  6.5390e-01,  4.5810e-01,  6.0330e-01,\n",
      "         4.3870e-01,  1.0907e+00,  2.7000e-01,  9.3790e-01,  8.9610e-01,\n",
      "         9.3400e-01,  3.5810e-01,  8.4580e-01,  6.8970e-01,  4.4740e-01,\n",
      "         6.3300e-01,  9.3390e-01,  3.0710e-01,  3.3720e-01, -8.2700e-02,\n",
      "         9.7320e-01,  5.9530e-01,  4.6980e-01,  5.4730e-01,  7.6960e-01,\n",
      "         1.5002e+00,  5.3390e-01,  1.2635e+00,  1.3200e-01,  4.7440e-01,\n",
      "         5.1840e-01,  2.0977e+00,  1.0565e+00,  7.8470e-01,  1.0683e+00,\n",
      "         5.7630e-01,  5.3020e-01,  1.1435e+00, -1.2170e-01,  1.4355e+00,\n",
      "         5.0720e-01,  9.5840e-01,  9.3410e-01]], dtype=float32)}) \n",
      "\n",
      "('f0e6446d-7f66-5425-b4bf-2820c05a069a', {'Sector': '\"325412\"', 'Revenue': [0.5, 0.5, 0.5, 0.4, 0.4], 'EBITDA': [0.1, 0.3, 0.1, 0, 0], 'NDE': [0, 1.0, 0, 1.0, 1.0], 'TDE': [0, 0, 0, 0, 0], 'Description Embedding': array([[1.373 , 1.5219, 1.3254, 0.6086, 1.9921, 0.7464, 0.7638, 1.3783,\n",
      "        1.2702, 0.8968, 0.8759, 0.8495, 0.8911, 1.4986, 0.839 , 1.5279,\n",
      "        1.7995, 1.2219, 0.9469, 1.3137, 0.4533, 0.8106, 1.4784, 1.5138,\n",
      "        1.2044, 1.0085, 0.947 , 1.018 , 0.4847, 0.8575, 1.8053, 0.9907,\n",
      "        0.7146, 0.8747, 1.8254, 1.3333, 0.5012, 0.9404, 0.9211, 0.9963,\n",
      "        0.579 , 0.856 , 0.5485, 1.0957, 0.8442, 0.7065, 1.8146, 1.2032,\n",
      "        1.6261, 1.7848, 1.0675, 0.9906, 0.7567, 0.6356, 1.0917, 1.3376,\n",
      "        0.8092, 0.6317, 0.6616, 0.5772, 1.5079, 0.8179, 1.679 , 0.7695,\n",
      "        2.2672, 1.4811, 1.2742, 0.8878, 0.529 , 1.0483, 0.8857, 1.335 ,\n",
      "        1.0473, 1.1395, 0.9026, 1.2378, 0.6654, 0.9742, 0.7921, 1.3259,\n",
      "        1.3657, 1.5424, 1.0227, 1.1461, 0.7169, 1.2821, 1.5653, 1.4144,\n",
      "        0.8122, 1.6085, 0.8743, 1.0417, 1.6044, 0.8017, 0.9481, 1.1912,\n",
      "        1.3349, 0.7216, 0.7477, 1.7915, 0.9344, 1.0898, 0.986 , 0.9079,\n",
      "        0.3837, 0.7593, 0.8746, 1.0018, 0.6426, 1.4347, 0.8238, 0.7417,\n",
      "        1.0974, 0.8136, 0.5517, 1.054 , 1.1022, 1.3927, 1.0101, 0.5978,\n",
      "        1.635 , 1.1636, 1.0831, 1.8946, 0.7304, 0.7786, 1.3772, 0.8504,\n",
      "        0.8669, 0.455 , 1.2679, 1.0131, 0.8616, 1.0199, 0.5647, 1.1302,\n",
      "        1.106 , 0.6988, 1.36  , 0.9661, 1.3938, 0.9343, 1.3288, 0.47  ,\n",
      "        1.0639, 0.5281, 1.0462, 0.9071, 0.6899, 1.5114, 1.2279, 1.2003,\n",
      "        0.5738, 0.9624, 1.0068, 1.1884, 1.3294, 1.2456, 1.1146, 0.6063,\n",
      "        1.1791, 1.3856, 0.8041, 1.3323, 0.5704, 1.1275, 1.1812, 1.5768,\n",
      "        0.6232, 0.8798, 1.035 , 0.6578, 1.8536, 0.6109, 1.7598, 1.1224,\n",
      "        1.2222, 0.8634, 1.3032, 1.0346, 0.4321, 1.65  , 1.2507, 1.002 ,\n",
      "        0.8986, 0.8226, 2.1164, 0.5177, 1.024 , 0.9204, 0.5601, 0.7173,\n",
      "        0.539 , 0.6777, 0.9379, 0.7409, 0.4893, 1.1424, 0.8056, 0.6063,\n",
      "        1.0796, 1.076 , 0.9647, 0.8046, 0.7096, 3.9095, 1.1353, 0.4214,\n",
      "        0.9062, 0.8108, 0.6991, 1.3229, 1.0003, 1.0728, 1.2244, 0.7006,\n",
      "        0.7745, 1.5675, 0.891 , 0.8186, 1.1692, 0.7614, 0.7445, 1.4049,\n",
      "        0.5599, 2.6879, 0.8114, 0.3498, 1.3881, 1.447 , 0.9284, 1.0832,\n",
      "        1.9314, 0.7228, 0.9939, 0.9016, 0.4156, 1.1639, 1.2131, 0.8302,\n",
      "        0.8371, 1.6947, 1.0657, 0.6378, 1.7871, 0.9349, 1.0067, 0.8388,\n",
      "        0.8067, 0.5539, 0.8228, 0.6277, 0.5474, 0.684 , 1.6166, 0.9519,\n",
      "        0.8624, 0.8234, 0.9149, 0.8235, 1.125 , 0.8432, 1.7621, 1.5532,\n",
      "        0.6397, 0.5974, 1.6776, 1.4552, 0.876 , 1.8548, 1.1615, 1.1471,\n",
      "        0.8577, 1.1515, 0.6154, 1.0068, 1.0947, 1.2663, 0.8146, 0.7049,\n",
      "        1.0134, 1.2334, 0.7091, 0.7607, 0.7361, 0.6985, 1.2037, 2.0653,\n",
      "        0.9572, 1.0615, 0.7827, 1.0134, 0.4429, 1.2671, 1.0094, 0.5376,\n",
      "        0.654 , 1.0358, 1.2298, 0.7896, 1.1142, 0.4776, 1.5299, 1.0921,\n",
      "        1.719 , 1.2921, 0.8152, 0.4884, 2.2044, 1.4445, 0.8976, 1.0608,\n",
      "        1.6781, 1.1191, 0.8848, 0.9044, 0.5184, 1.2923, 1.5988, 0.4794,\n",
      "        1.5218, 1.2343, 1.1471, 0.9133, 1.1029, 0.6911, 1.131 , 1.3242,\n",
      "        0.9465, 0.2819, 0.8772, 1.0906, 1.7093, 1.5411, 1.235 , 0.9122,\n",
      "        0.9084, 0.9639, 0.6392, 0.501 , 0.9139, 1.5751, 1.0229, 1.0948,\n",
      "        0.7111, 0.6396, 1.687 , 0.3968, 1.1292, 0.8529, 1.2575, 0.5984,\n",
      "        1.9626, 0.7561, 1.3912, 1.1269, 1.6174, 1.5349, 1.3937, 1.1304,\n",
      "        0.8231, 1.1211, 1.1609, 0.7022, 1.1718, 1.5455, 0.8522, 1.3398,\n",
      "        1.4225, 1.0661, 0.6381, 0.6997, 1.9911, 1.0647, 0.6927, 0.671 ,\n",
      "        1.3166, 0.8315, 1.1133, 1.5705, 0.8181, 0.5677, 1.0731, 0.7137,\n",
      "        1.3346, 0.6627, 1.345 , 0.8085, 0.6824, 0.817 , 1.7387, 1.512 ,\n",
      "        0.8996, 0.5661, 0.5615, 1.9081, 0.5634, 0.5913, 1.4021, 1.7772,\n",
      "        1.3447, 0.9067, 1.5913, 0.6921, 0.9651, 1.4996, 0.9363, 1.0236,\n",
      "        1.0644, 1.1554, 1.5434, 0.4818, 0.7579, 0.9655, 0.7019, 0.6756,\n",
      "        1.5239, 0.956 , 0.7231, 0.8142, 1.2857, 0.457 , 1.1836, 1.1407,\n",
      "        0.9354, 1.7316, 0.7278, 0.6402, 0.8122, 1.4514, 0.8114, 1.2966,\n",
      "        1.7301, 0.7593, 0.5248, 1.7278, 0.7523, 0.7735, 1.3812, 1.1374,\n",
      "        1.4642, 1.1624, 1.2947, 1.3678, 0.9021, 0.8446, 0.8487, 1.6938,\n",
      "        1.2692, 1.6272, 0.8882, 0.7941, 1.2374, 1.3999, 1.5988, 1.2983,\n",
      "        0.5693, 1.135 , 1.1288, 0.7265, 0.7198, 0.9336, 1.3027, 0.9076,\n",
      "        1.5164, 0.6103, 1.0396, 0.5873, 0.902 , 1.189 , 1.2683, 0.8286,\n",
      "        0.731 , 1.757 , 0.5817, 0.8153, 1.1748, 1.1421, 1.2262, 0.7017,\n",
      "        1.183 , 0.9283, 0.6168, 1.46  , 0.8674, 0.9832, 1.021 , 1.4692,\n",
      "        1.1926, 0.4302, 0.3905, 1.0825, 1.2208, 0.8137, 1.4237, 0.6532,\n",
      "        0.8758, 0.7462, 1.3171, 1.3784, 0.518 , 1.1979, 0.814 , 0.5362,\n",
      "        1.1882, 1.4983, 0.9491, 1.4423, 0.8342, 1.0162, 0.7868, 1.2839,\n",
      "        1.119 , 0.6072, 1.1811, 1.4544, 1.3431, 1.0015, 0.6849, 0.3637,\n",
      "        1.4181, 1.0506, 0.9755, 1.4959, 0.9308, 1.0087, 0.4599, 1.1862,\n",
      "        0.5483, 0.4665, 1.4056, 0.8362, 0.8315, 1.4558, 0.7835, 1.0542,\n",
      "        0.6874, 0.7984, 1.0669, 0.7578, 0.7984, 0.9459, 1.3959, 1.3999,\n",
      "        0.7124, 0.6469, 1.3586, 0.7444, 0.7561, 0.7766, 0.7408, 1.0633,\n",
      "        1.1573, 1.349 , 1.2926, 1.4005, 0.8817, 1.4773, 0.4437, 0.7156,\n",
      "        0.8097, 1.1521, 1.2344, 0.4969, 0.5908, 0.9388, 0.4077, 0.6105,\n",
      "        0.5694, 1.769 , 0.9586, 0.9061, 1.4387, 0.7844, 0.5166, 1.593 ,\n",
      "        1.1847, 0.7268, 0.9935, 0.7902, 0.972 , 1.0231, 0.7198, 1.1035,\n",
      "        1.6292, 0.5199, 1.114 , 1.1314, 1.9636, 0.9183, 0.992 , 0.9805,\n",
      "        2.0117, 0.8595, 1.0203, 1.1927, 0.9056, 0.5995, 1.2658, 1.1666,\n",
      "        1.1947, 1.5691, 0.4686, 0.8962, 1.6666, 0.7548, 1.1142, 0.9805,\n",
      "        0.9923, 1.3729, 1.3424, 0.8166, 0.7324, 1.3826, 1.1327, 0.4135,\n",
      "        0.8874, 1.5144, 1.4144, 0.4567, 1.6416, 1.1983, 1.0244, 1.0458,\n",
      "        0.8849, 0.9694, 1.149 , 0.7802, 1.1417, 0.8839, 1.5853, 0.6183,\n",
      "        1.499 , 1.5437, 0.9538, 0.7713, 0.8014, 1.1068, 0.8056, 0.7528,\n",
      "        0.8008, 1.019 , 1.3315, 0.8599, 0.6527, 1.4132, 1.3816, 1.0163,\n",
      "        0.9131, 1.0131, 1.2697, 1.303 , 1.2184, 0.9932, 1.2998, 1.1501,\n",
      "        1.3366, 1.5285, 1.7758, 1.0022, 1.3912, 1.7077, 1.7912, 1.6545,\n",
      "        1.7841, 1.184 , 0.9057, 1.4853, 0.9045, 1.1997, 2.0525, 0.4426,\n",
      "        0.8825, 1.0103, 1.5199, 0.4812, 0.954 , 1.114 , 1.0868, 1.1768,\n",
      "        0.6049, 1.2361, 1.0477, 1.3766, 1.1112, 1.2461, 1.358 , 1.4732,\n",
      "        0.8102, 0.6477, 0.4276, 0.823 , 1.8327, 0.6976, 0.829 , 1.0372,\n",
      "        1.0708, 0.9848, 1.1033, 0.6389, 1.3978, 0.8432, 1.6979, 1.0939,\n",
      "        1.7842, 0.923 , 1.1465, 0.7188, 1.1876, 1.3631, 0.59  , 1.2271,\n",
      "        1.3962, 0.8284, 1.125 , 1.3868, 0.8478, 0.6587, 1.3991, 1.2239,\n",
      "        0.4409, 1.1214, 0.8882, 1.2352, 0.9651, 1.3257, 1.2097, 1.1289,\n",
      "        0.7086, 0.7878, 0.7658, 1.0588, 0.5248, 0.8365, 1.6008, 1.3682,\n",
      "        1.0663, 0.9186, 1.0577, 0.931 , 0.9397, 1.6744, 0.6882, 0.6839,\n",
      "        0.6058, 1.2698, 0.7836, 1.0202, 0.9769, 1.895 , 1.5072, 0.8484,\n",
      "        1.8192, 0.417 , 0.5272, 0.6209, 1.7457, 0.8156, 1.0279, 0.9593,\n",
      "        1.0468, 0.9714, 0.9363, 0.3395, 1.7602, 0.7531, 1.0002, 1.0046]],\n",
      "      dtype=float32)}) \n",
      "\n",
      "('f3fc74d2-a9ae-5cfd-8a41-a510086c1781', {'Sector': '\"325412\"', 'Revenue': [0.5, 0.5, 0.7, 0.5, 0.5], 'EBITDA': [0, 0, 0.6, 0, 0], 'NDE': [0, 0, 0, 0, 0], 'TDE': [0.87, 0.71, 0.86, 0, 0], 'Description Embedding': array([[0.6979, 1.0116, 0.8939, 0.5742, 1.5059, 0.7463, 0.545 , 1.0351,\n",
      "        1.0806, 0.7752, 0.6482, 0.3593, 0.9191, 1.5509, 0.5172, 1.8671,\n",
      "        0.5472, 1.2588, 1.1527, 0.7793, 1.5536, 0.3315, 0.9593, 1.7058,\n",
      "        1.0334, 0.6928, 1.0285, 0.9506, 0.5168, 0.5734, 1.2864, 1.1021,\n",
      "        0.6494, 0.829 , 1.0828, 1.0206, 0.3056, 1.1954, 0.4072, 0.9381,\n",
      "        0.4133, 0.5697, 0.268 , 1.2181, 0.6654, 0.7577, 1.1356, 0.4346,\n",
      "        1.0671, 0.7757, 0.5227, 0.6761, 0.5662, 0.3699, 0.8451, 1.3319,\n",
      "        0.6822, 0.4768, 0.4623, 0.5524, 1.2934, 0.658 , 0.5853, 0.8967,\n",
      "        1.4491, 1.0842, 0.9253, 1.1854, 0.1923, 1.3518, 0.7096, 1.0091,\n",
      "        0.8588, 0.8016, 0.865 , 0.484 , 0.9122, 1.2592, 0.8393, 0.8426,\n",
      "        1.0638, 1.3896, 0.967 , 1.5206, 0.5043, 1.3691, 1.0249, 1.2466,\n",
      "        0.4556, 1.4831, 1.0277, 0.7703, 0.895 , 0.6397, 1.5975, 0.6673,\n",
      "        0.783 , 0.6231, 1.2452, 1.747 , 1.0983, 0.5194, 0.5691, 0.7306,\n",
      "        0.5541, 0.6328, 0.7568, 0.6798, 0.854 , 1.3332, 1.3818, 0.6922,\n",
      "        0.586 , 0.4058, 0.4103, 1.325 , 0.6508, 1.2025, 0.85  , 0.5196,\n",
      "        0.8714, 1.47  , 0.6319, 2.0982, 0.3772, 0.7113, 0.8904, 1.0155,\n",
      "        0.9452, 1.1333, 1.3599, 1.3029, 0.6842, 0.5687, 0.613 , 0.8586,\n",
      "        1.2516, 0.354 , 1.1245, 0.9583, 0.7779, 0.6449, 1.6394, 1.3228,\n",
      "        0.9188, 0.4394, 0.8534, 1.3804, 1.0056, 1.3428, 1.378 , 1.2338,\n",
      "        0.4043, 0.5665, 0.8657, 0.6023, 0.6553, 1.1562, 0.9458, 0.6852,\n",
      "        0.966 , 0.9138, 0.437 , 0.937 , 0.3256, 1.0959, 1.3502, 1.1404,\n",
      "        0.6115, 1.0406, 0.8788, 0.3251, 1.3462, 0.4293, 1.0227, 0.93  ,\n",
      "        0.8336, 0.4186, 0.68  , 1.0307, 0.1629, 1.7637, 1.2541, 0.9019,\n",
      "        1.0965, 0.7207, 2.0356, 0.7877, 1.1949, 1.1057, 0.1684, 0.8752,\n",
      "        0.701 , 0.796 , 1.0407, 0.6016, 0.7295, 1.5134, 0.5673, 0.3034,\n",
      "        0.6058, 1.0311, 1.125 , 0.6027, 0.7593, 3.5651, 1.1061, 0.5558,\n",
      "        0.5854, 0.7297, 0.8443, 0.9461, 1.6539, 0.6201, 1.4006, 0.4659,\n",
      "        0.5385, 1.107 , 0.5451, 0.8048, 0.9655, 1.2001, 0.5432, 1.198 ,\n",
      "        0.5746, 1.9207, 0.7281, 0.6152, 1.1804, 1.5983, 0.4181, 0.8768,\n",
      "        1.6261, 0.3551, 0.6154, 0.7604, 0.2651, 0.5812, 1.0829, 0.6928,\n",
      "        0.4721, 0.8616, 0.657 , 0.7661, 2.1037, 0.4647, 0.8643, 0.9676,\n",
      "        0.608 , 0.2533, 0.2668, 0.7361, 0.6233, 0.6762, 1.2915, 0.5839,\n",
      "        0.6603, 0.901 , 0.6057, 0.8206, 0.9743, 0.9694, 1.7546, 0.7463,\n",
      "        0.8223, 0.4954, 1.1979, 0.718 , 1.064 , 0.7296, 1.0981, 0.7947,\n",
      "        0.9142, 1.245 , 0.5277, 0.3691, 0.773 , 1.3381, 0.6575, 0.8983,\n",
      "        0.992 , 1.1654, 0.6927, 0.6496, 0.7034, 0.5943, 1.2067, 2.0783,\n",
      "        0.5051, 1.0466, 0.5713, 0.6685, 0.3818, 1.2424, 0.6075, 0.5581,\n",
      "        0.7394, 1.6809, 0.9537, 0.5938, 0.7616, 0.4171, 1.4047, 1.0702,\n",
      "        0.5884, 0.601 , 1.3089, 0.4796, 1.7931, 0.5326, 0.7497, 0.7279,\n",
      "        1.9557, 0.5283, 1.1247, 1.0483, 0.588 , 0.7899, 1.0196, 0.6002,\n",
      "        0.8409, 0.4309, 1.2338, 1.0581, 1.1935, 0.5113, 0.9402, 1.0346,\n",
      "        0.7264, 0.2902, 0.7111, 0.9517, 0.9   , 0.4719, 0.3885, 1.0182,\n",
      "        0.3908, 0.2623, 1.2452, 0.3001, 0.7785, 1.2974, 1.1309, 0.6855,\n",
      "        0.6628, 0.6181, 0.8595, 0.4658, 1.0098, 0.7763, 0.5306, 0.3944,\n",
      "        1.5715, 0.4765, 1.1131, 1.4223, 0.6464, 1.0166, 0.8845, 0.3247,\n",
      "        0.919 , 0.517 , 1.5449, 0.4401, 0.8588, 1.4819, 0.3323, 1.242 ,\n",
      "        0.7452, 0.8237, 0.5133, 0.4531, 0.8958, 0.7782, 0.5414, 0.443 ,\n",
      "        0.7298, 0.6794, 0.9956, 1.2354, 0.8793, 0.2322, 1.196 , 0.6394,\n",
      "        1.2081, 1.0498, 0.6811, 0.9254, 0.3704, 0.944 , 1.2199, 0.6321,\n",
      "        0.8635, 0.4139, 0.5478, 1.052 , 0.3383, 1.06  , 1.4235, 1.1367,\n",
      "        1.1702, 1.0364, 1.0925, 0.5793, 0.6271, 1.4446, 1.0757, 0.7967,\n",
      "        1.0823, 0.6945, 1.5247, 0.5309, 0.8759, 0.6576, 0.458 , 0.2622,\n",
      "        1.3272, 0.8308, 0.879 , 0.8734, 1.3504, 0.6057, 1.4129, 0.8343,\n",
      "        0.9683, 1.2343, 0.9117, 0.6593, 0.5739, 1.3278, 0.6316, 0.6195,\n",
      "        1.0626, 0.7768, 0.2956, 0.9029, 0.3759, 0.7284, 1.3253, 1.2392,\n",
      "        0.6501, 1.1524, 1.2139, 0.6287, 0.9132, 0.7386, 0.6126, 1.114 ,\n",
      "        0.9991, 0.9433, 1.0185, 1.1708, 1.073 , 0.6661, 1.1077, 1.0703,\n",
      "        0.9579, 0.9675, 0.9081, 0.9233, 0.4812, 1.5925, 1.3381, 1.4   ,\n",
      "        1.0894, 0.6918, 0.7496, 0.563 , 1.01  , 1.254 , 1.4043, 0.7104,\n",
      "        0.5401, 1.4438, 0.6813, 0.9599, 1.0373, 0.5761, 0.9546, 0.535 ,\n",
      "        0.7481, 0.7168, 0.584 , 1.2205, 0.947 , 0.7606, 0.739 , 1.5136,\n",
      "        0.6762, 0.7938, 0.4624, 0.9228, 1.3496, 0.9665, 1.1915, 0.3477,\n",
      "        0.9568, 1.036 , 1.0155, 0.9508, 0.9082, 1.0242, 0.7573, 0.5453,\n",
      "        1.0147, 1.0619, 0.8663, 0.782 , 1.3568, 0.8857, 0.8489, 0.7397,\n",
      "        0.713 , 0.5859, 0.676 , 1.0697, 0.6528, 1.5056, 0.5944, 0.2991,\n",
      "        0.5891, 0.8725, 0.8383, 1.1014, 1.0109, 0.5349, 0.5536, 1.068 ,\n",
      "        0.8972, 0.782 , 1.3869, 0.662 , 0.9071, 1.0319, 1.0048, 1.2944,\n",
      "        0.3924, 0.8045, 0.3601, 1.0298, 0.8122, 0.49  , 1.426 , 0.9635,\n",
      "        1.0492, 0.8547, 0.9102, 0.5324, 0.316 , 0.6777, 0.5296, 0.5301,\n",
      "        0.9039, 1.0409, 1.7982, 1.4373, 0.4692, 1.5118, 0.411 , 0.4853,\n",
      "        0.6909, 1.0026, 0.7435, 0.4303, 0.7056, 0.5825, 0.3673, 0.5171,\n",
      "        0.5991, 1.0876, 0.7348, 0.7883, 1.4144, 0.795 , 0.4619, 1.0355,\n",
      "        1.1061, 0.6609, 0.7991, 0.9119, 0.739 , 0.91  , 0.8133, 1.1281,\n",
      "        1.2115, 0.7063, 1.3231, 0.7798, 2.037 , 0.8443, 1.2224, 0.7991,\n",
      "        1.924 , 0.4633, 0.5417, 1.2845, 0.8129, 0.3419, 1.1773, 0.6629,\n",
      "        0.4562, 1.2408, 0.6426, 1.4084, 1.5269, 1.2145, 1.4085, 1.2553,\n",
      "        1.1303, 0.9753, 1.3055, 0.6909, 0.831 , 1.499 , 1.1942, 0.6775,\n",
      "        1.0393, 0.8533, 0.6004, 0.7996, 1.7402, 1.391 , 0.6197, 0.7396,\n",
      "        0.302 , 0.5508, 0.8413, 0.7191, 0.5335, 1.0831, 0.9443, 0.3238,\n",
      "        1.0742, 1.4889, 0.5263, 0.8162, 0.9024, 1.7099, 0.6855, 1.1025,\n",
      "        0.5543, 1.4222, 0.7661, 0.9271, 0.7263, 1.6344, 0.6029, 0.5818,\n",
      "        1.4821, 0.6755, 1.0817, 1.6732, 1.4858, 0.9805, 1.1465, 0.9439,\n",
      "        1.0707, 2.1131, 0.8394, 0.8633, 1.2008, 1.671 , 1.135 , 1.1208,\n",
      "        1.2637, 1.0299, 0.6347, 1.7684, 0.8836, 0.8483, 1.7131, 0.5055,\n",
      "        0.7401, 0.9247, 1.675 , 0.2936, 0.7941, 0.87  , 0.8514, 1.0455,\n",
      "        0.4364, 1.0526, 0.6627, 1.7952, 0.8372, 0.5436, 0.5787, 0.8766,\n",
      "        0.5689, 0.5168, 0.2412, 0.2808, 1.1327, 1.1262, 0.8903, 0.5351,\n",
      "        0.7904, 0.5398, 0.9898, 0.9576, 1.5443, 0.7743, 0.5129, 0.9761,\n",
      "        1.6948, 0.9224, 0.8889, 0.8758, 1.3008, 0.9841, 0.5969, 0.9724,\n",
      "        0.7288, 0.9031, 1.0177, 1.0646, 0.5536, 0.3111, 1.033 , 1.1612,\n",
      "        0.4756, 1.0523, 0.7014, 1.0006, 0.7856, 1.0732, 1.0944, 1.1262,\n",
      "        0.8145, 0.7861, 1.0069, 0.8673, 0.2378, 0.9271, 1.1889, 1.0073,\n",
      "        0.6576, 0.876 , 0.6319, 0.8856, 0.7496, 0.9631, 0.5684, 0.5355,\n",
      "        0.4637, 0.8723, 0.5286, 0.4927, 0.8556, 1.1977, 2.0006, 0.729 ,\n",
      "        1.2927, 0.8224, 0.349 , 0.5586, 0.8578, 0.983 , 0.5164, 0.8462,\n",
      "        1.1678, 0.8024, 1.0374, 0.5218, 0.9767, 0.43  , 1.2641, 0.9383]],\n",
      "      dtype=float32)}) \n",
      "\n",
      "('f43889b6-3216-52a1-8870-0dd5cef34d5b', {'Sector': '\"511210\"', 'Revenue': [0.5, 0.4, 0.3, 0.2, 0.2], 'EBITDA': [0, 0, 0, 0, 0], 'NDE': [0, 1.0, 0, 1.0, 0], 'TDE': [0, 0.37, 0.01, 0.77, 0.94], 'Description Embedding': array([[1.0231, 1.053 , 1.2382, 1.0495, 1.7857, 0.6054, 0.552 , 1.351 ,\n",
      "        1.5849, 0.52  , 0.5108, 0.4415, 1.0229, 1.4673, 0.717 , 1.0857,\n",
      "        0.7677, 0.9792, 0.9042, 1.5712, 0.7613, 0.378 , 1.4032, 1.6979,\n",
      "        1.0763, 1.1449, 0.6501, 0.6499, 0.6273, 0.8059, 1.4264, 0.8558,\n",
      "        1.2939, 0.7765, 1.0168, 0.7965, 0.3916, 0.6542, 0.2156, 0.8143,\n",
      "        0.4415, 0.597 , 1.504 , 0.9451, 1.1395, 0.589 , 0.8717, 0.5807,\n",
      "        1.4427, 0.8229, 0.2719, 1.2507, 0.8957, 0.363 , 1.4637, 1.5143,\n",
      "        1.0645, 0.6358, 0.523 , 0.3392, 0.979 , 0.6641, 0.715 , 0.8998,\n",
      "        1.6784, 0.7161, 1.4408, 1.2495, 0.1402, 0.8278, 1.5415, 0.9222,\n",
      "        0.7462, 0.7971, 0.5742, 0.7479, 0.387 , 1.37  , 0.746 , 1.0732,\n",
      "        1.1081, 1.5094, 0.863 , 1.6214, 1.0007, 1.4853, 1.0707, 0.8716,\n",
      "        0.4884, 1.4568, 1.07  , 0.8174, 0.9764, 0.8825, 0.8926, 0.6342,\n",
      "        0.8973, 0.5239, 0.9575, 1.335 , 1.0051, 0.9307, 0.7109, 0.555 ,\n",
      "        0.6392, 0.7196, 0.9381, 0.8731, 0.5909, 1.0581, 0.7744, 0.628 ,\n",
      "        1.041 , 0.4586, 0.6927, 1.1608, 0.7658, 1.2064, 1.0533, 0.5811,\n",
      "        1.1934, 1.4663, 1.1415, 1.7675, 0.5392, 1.1771, 0.8796, 0.8337,\n",
      "        0.9654, 0.8747, 1.3382, 0.9874, 0.6536, 0.8691, 0.6483, 1.2088,\n",
      "        0.7164, 0.2143, 1.5963, 0.9984, 0.9231, 1.0368, 1.3225, 1.1265,\n",
      "        0.7261, 0.4844, 1.4391, 0.6104, 0.6113, 1.3223, 1.1284, 1.0663,\n",
      "        0.4424, 1.0748, 0.7192, 0.5698, 1.0714, 0.8172, 0.953 , 0.5044,\n",
      "        1.6146, 1.0507, 1.3843, 1.8706, 0.426 , 0.3954, 0.7923, 1.3505,\n",
      "        0.4549, 0.8468, 0.9661, 0.9157, 1.5692, 0.6028, 1.922 , 1.1939,\n",
      "        0.9019, 1.0889, 1.4981, 0.8933, 0.0102, 1.1616, 1.4019, 0.687 ,\n",
      "        1.5505, 0.8968, 2.2615, 0.7602, 1.3371, 0.7434, 0.4579, 0.2088,\n",
      "        1.0295, 0.6278, 0.7842, 0.363 , 0.8639, 0.5468, 0.8224, 0.6771,\n",
      "        0.9745, 1.0259, 0.7799, 0.8284, 0.4949, 3.6535, 0.732 , 0.3394,\n",
      "        0.8809, 0.9775, 0.7741, 0.8141, 0.7973, 1.0136, 1.3826, 0.2652,\n",
      "        0.4381, 0.9534, 1.2937, 0.8523, 1.7716, 0.5388, 0.6103, 1.2969,\n",
      "        0.6739, 1.6257, 1.1143, 0.3628, 1.3438, 1.6255, 0.8128, 0.8463,\n",
      "        1.3567, 0.3238, 0.9561, 0.7866, 0.6514, 0.4789, 0.9635, 0.7788,\n",
      "        0.4644, 1.6817, 1.2686, 0.5688, 1.5024, 0.5142, 0.8352, 0.7604,\n",
      "        0.5701, 0.7957, 0.5732, 0.8665, 0.6114, 0.1746, 1.5656, 0.7597,\n",
      "        0.6545, 1.1524, 0.8251, 0.732 , 0.9187, 1.1167, 2.3291, 1.0605,\n",
      "        0.9468, 0.5148, 0.9832, 0.9112, 0.9127, 0.9685, 1.1352, 0.641 ,\n",
      "        0.6149, 1.1628, 0.8324, 0.6   , 1.0714, 1.1551, 0.5637, 0.9462,\n",
      "        1.3524, 1.3527, 0.5383, 0.9652, 1.0013, 1.0144, 1.0713, 1.0978,\n",
      "        0.6675, 1.2387, 0.7507, 0.9063, 0.2131, 0.8916, 1.2242, 0.8924,\n",
      "        0.7097, 1.4535, 1.317 , 1.1184, 0.6912, 0.4403, 1.3446, 0.8232,\n",
      "        0.904 , 0.5491, 0.8532, 0.3122, 3.6135, 0.6704, 0.9792, 0.511 ,\n",
      "        1.3556, 0.3761, 1.2522, 0.6251, 0.5961, 1.1149, 1.7848, 0.515 ,\n",
      "        0.8323, 0.5519, 0.4436, 1.2569, 1.2356, 0.7839, 0.904 , 1.2274,\n",
      "        0.6743, 0.1166, 1.0962, 0.719 , 1.3464, 1.5073, 0.9822, 1.1703,\n",
      "        0.4504, 0.5872, 1.3988, 0.5666, 0.6744, 1.5742, 0.5699, 0.407 ,\n",
      "        0.7649, 0.6892, 1.4239, 0.2876, 1.2551, 0.7929, 0.7719, 1.1975,\n",
      "        1.6197, 0.3576, 1.1377, 1.1866, 0.9436, 1.5158, 1.3586, 0.8285,\n",
      "        0.6864, 0.4239, 0.6933, 0.647 , 0.947 , 1.3325, 0.396 , 1.213 ,\n",
      "        1.3355, 0.8584, 0.6627, 0.4958, 0.9164, 0.5519, 0.5977, 0.5683,\n",
      "        0.7045, 0.3742, 1.0235, 1.3638, 1.1687, 0.1582, 1.1812, 0.3632,\n",
      "        1.3057, 0.7384, 0.8765, 0.4379, 0.7361, 0.9485, 1.4074, 0.9268,\n",
      "        0.9596, 0.8444, 0.7167, 0.7594, 0.5915, 0.5648, 1.5429, 1.5493,\n",
      "        1.3484, 0.9254, 1.322 , 1.2397, 1.1587, 0.8577, 0.851 , 0.1526,\n",
      "        0.8324, 1.1641, 1.5474, 0.3458, 0.49  , 1.0557, 0.5839, 0.3837,\n",
      "        1.0121, 0.8444, 1.0636, 0.7962, 1.1265, 0.862 , 1.3267, 1.0411,\n",
      "        0.9778, 1.3565, 0.8247, 0.6559, 0.9415, 1.3692, 1.1367, 0.7667,\n",
      "        0.6841, 1.0422, 1.1159, 2.1327, 1.3274, 0.3638, 1.4858, 0.7207,\n",
      "        1.2319, 1.2259, 0.7082, 0.6019, 0.8809, 0.7705, 0.5431, 0.5482,\n",
      "        0.7658, 1.9347, 0.7714, 1.0475, 1.5805, 1.0925, 0.7223, 1.3245,\n",
      "        0.8647, 1.1425, 0.8079, 0.3657, 1.0631, 1.0932, 0.8676, 0.9268,\n",
      "        1.1806, 0.8606, 0.5954, 0.4174, 1.2098, 0.8637, 1.3591, 0.7282,\n",
      "        1.1324, 1.7157, 0.1844, 1.1456, 0.9881, 0.7432, 1.1592, 0.8611,\n",
      "        1.3587, 0.6672, 0.6679, 1.145 , 1.1815, 0.8301, 0.4084, 1.3517,\n",
      "        0.9529, 0.6044, 0.3266, 0.9844, 0.9932, 1.4625, 1.1028, 0.6083,\n",
      "        0.644 , 1.1371, 1.1896, 1.4215, 1.0505, 1.1468, 0.7634, 0.3542,\n",
      "        1.366 , 1.2069, 0.6195, 0.8348, 0.7388, 1.2364, 0.419 , 0.85  ,\n",
      "        0.7898, 0.5047, 1.8881, 0.8022, 1.0684, 1.4748, 0.4335, 0.7319,\n",
      "        0.9101, 0.5002, 0.7811, 1.1682, 1.3575, 0.5145, 0.6279, 1.1961,\n",
      "        0.504 , 0.5046, 0.6148, 0.8408, 0.5809, 1.0179, 0.6502, 0.9441,\n",
      "        0.479 , 0.7387, 0.4519, 0.8653, 0.8787, 0.546 , 1.1401, 1.1907,\n",
      "        0.7292, 0.8599, 1.184 , 0.7355, 0.164 , 0.6882, 0.6717, 0.666 ,\n",
      "        0.8815, 0.8591, 1.1013, 1.3935, 1.0351, 1.7518, 0.9194, 0.5966,\n",
      "        0.7679, 0.964 , 1.1641, 0.4784, 0.5941, 0.7729, 0.1565, 0.4168,\n",
      "        0.5168, 0.8525, 0.9075, 0.9629, 1.3384, 0.9645, 0.8131, 0.9127,\n",
      "        0.9195, 0.6956, 1.4813, 0.8851, 1.2628, 0.98  , 0.9838, 0.6297,\n",
      "        1.4763, 0.3845, 1.046 , 0.8518, 2.0959, 1.1008, 0.9608, 0.8108,\n",
      "        1.8717, 0.3224, 1.1523, 1.02  , 1.1265, 0.668 , 0.8025, 0.9139,\n",
      "        0.8149, 1.8782, 0.1761, 0.8003, 1.6015, 0.7244, 1.1303, 0.8692,\n",
      "        0.8334, 1.2921, 1.2526, 0.8724, 0.3582, 0.8986, 0.7744, 0.2228,\n",
      "        0.5312, 0.8712, 0.6196, 0.4188, 1.4182, 1.7829, 0.5183, 0.785 ,\n",
      "        0.581 , 0.6798, 1.1963, 0.914 , 0.8779, 1.1092, 0.9329, 1.249 ,\n",
      "        1.0479, 1.5435, 0.5467, 0.789 , 1.3134, 0.8023, 0.8308, 0.8668,\n",
      "        0.991 , 1.6106, 0.9195, 1.2267, 0.7453, 1.751 , 0.5122, 0.8193,\n",
      "        1.4707, 0.8979, 1.6043, 1.4295, 0.919 , 0.9984, 1.472 , 0.9219,\n",
      "        1.3662, 1.067 , 1.1391, 1.2669, 1.1437, 1.698 , 1.1516, 0.9982,\n",
      "        1.3018, 1.1238, 0.9688, 1.5121, 1.1503, 0.6449, 2.7462, 0.418 ,\n",
      "        0.56  , 0.8252, 1.084 , 0.2487, 0.6226, 0.7566, 1.5782, 1.2541,\n",
      "        0.292 , 0.7412, 1.3361, 1.3078, 0.9581, 0.6141, 1.2785, 1.226 ,\n",
      "        0.5527, 0.7915, 1.1661, 0.4341, 1.4992, 1.4625, 0.4559, 0.9948,\n",
      "        1.3682, 0.5007, 0.7344, 0.3662, 1.5757, 0.866 , 1.6558, 0.6829,\n",
      "        1.4027, 0.8447, 1.6793, 0.8613, 1.6975, 1.3169, 0.8308, 0.9049,\n",
      "        0.7926, 1.1421, 1.2458, 1.353 , 0.4093, 0.5659, 1.1065, 1.3718,\n",
      "        0.5604, 1.0727, 1.0084, 0.8307, 0.9744, 1.1237, 0.9214, 0.8164,\n",
      "        0.4981, 0.4356, 1.4583, 0.6085, 0.9393, 1.2008, 1.1788, 1.0011,\n",
      "        1.0165, 1.1249, 0.5504, 0.8282, 0.8744, 1.6299, 0.336 , 0.6825,\n",
      "        0.2979, 1.044 , 0.7908, 0.8123, 0.6174, 1.0532, 0.9652, 1.4855,\n",
      "        1.1772, 0.7495, 0.3624, 0.4755, 1.8682, 1.0916, 0.9606, 0.6389,\n",
      "        1.0605, 0.7015, 0.8376, 0.7722, 0.812 , 0.6853, 1.1544, 0.9863]],\n",
      "      dtype=float32)}) \n",
      "\n",
      "Stored 'Final_Data_Set_Nor' (dict)\n"
     ]
    }
   ],
   "source": [
    "Final_Data_Set_Nor = {}\n",
    "\n",
    "for i in EN_Des.keys():\n",
    "    \n",
    "    Final_Data_Set_Nor[i] = {}\n",
    "    Final_Data_Set_Nor[i].update(Data_Set_N[i])\n",
    "    Final_Data_Set_Nor[i].update({ 'Description Embedding' : EN_Des[i]['Description Emb']})\n",
    "    \n",
    "    \n",
    "n = 0\n",
    "for i in Final_Data_Set_Nor.items():\n",
    "    print(i,\"\\n\")\n",
    "    n += 1\n",
    "    if n == 5:\n",
    "        break\n",
    "\n",
    "%store Final_Data_Set_Nor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f14004b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756\n"
     ]
    }
   ],
   "source": [
    "print(len(Final_Data_Set_Nor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f24fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
